{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import unique\n",
    "from numpy import where\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = pd.read_csv(\"data/data.csv\",  header=None)\n",
    "target = pd.read_csv(\"data/id.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "INFO data\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 2450 entries, 0 to 2449\nData columns (total 128 columns):\n #   Column  Dtype  \n---  ------  -----  \n 0   0       float64\n 1   1       float64\n 2   2       float64\n 3   3       float64\n 4   4       float64\n 5   5       float64\n 6   6       float64\n 7   7       float64\n 8   8       float64\n 9   9       float64\n 10  10      float64\n 11  11      float64\n 12  12      float64\n 13  13      float64\n 14  14      float64\n 15  15      float64\n 16  16      float64\n 17  17      float64\n 18  18      float64\n 19  19      float64\n 20  20      float64\n 21  21      float64\n 22  22      float64\n 23  23      float64\n 24  24      float64\n 25  25      float64\n 26  26      float64\n 27  27      float64\n 28  28      float64\n 29  29      float64\n 30  30      float64\n 31  31      float64\n 32  32      float64\n 33  33      float64\n 34  34      float64\n 35  35      float64\n 36  36      float64\n 37  37      float64\n 38  38      float64\n 39  39      float64\n 40  40      float64\n 41  41      float64\n 42  42      float64\n 43  43      float64\n 44  44      float64\n 45  45      float64\n 46  46      float64\n 47  47      float64\n 48  48      float64\n 49  49      float64\n 50  50      float64\n 51  51      float64\n 52  52      float64\n 53  53      float64\n 54  54      float64\n 55  55      float64\n 56  56      float64\n 57  57      float64\n 58  58      float64\n 59  59      float64\n 60  60      float64\n 61  61      float64\n 62  62      float64\n 63  63      float64\n 64  64      float64\n 65  65      float64\n 66  66      float64\n 67  67      float64\n 68  68      float64\n 69  69      float64\n 70  70      float64\n 71  71      float64\n 72  72      float64\n 73  73      float64\n 74  74      float64\n 75  75      float64\n 76  76      float64\n 77  77      float64\n 78  78      float64\n 79  79      float64\n 80  80      float64\n 81  81      float64\n 82  82      float64\n 83  83      float64\n 84  84      float64\n 85  85      float64\n 86  86      float64\n 87  87      float64\n 88  88      float64\n 89  89      float64\n 90  90      float64\n 91  91      float64\n 92  92      float64\n 93  93      float64\n 94  94      float64\n 95  95      float64\n 96  96      float64\n 97  97      float64\n 98  98      float64\n 99  99      float64\n 100 100     float64\n 101 101     float64\n 102 102     float64\n 103 103     float64\n 104 104     float64\n 105 105     float64\n 106 106     float64\n 107 107     float64\n 108 108     float64\n 109 109     float64\n 110 110     float64\n 111 111     float64\n 112 112     float64\n 113 113     float64\n 114 114     float64\n 115 115     float64\n 116 116     float64\n 117 117     float64\n 118 118     float64\n 119 119     float64\n 120 120     float64\n 121 121     float64\n 122 122     float64\n 123 123     float64\n 124 124     float64\n 125 125     float64\n 126 126     float64\n 127 127     float64\ndtypes: float64(128)\nmemory usage: 2.4 MB\nNone\nINFO target\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 2450 entries, 0 to 2449\nData columns (total 1 columns):\n #   Column  Non-Null Count  Dtype\n---  ------  --------------  -----\n 0   0       2450 non-null   int64\ndtypes: int64(1)\nmemory usage: 19.3 KB\nNone\n"
    }
   ],
   "source": [
    "#print(data.head)\n",
    "print(\"INFO data\")\n",
    "print(data.info(verbose=True))\n",
    "print(\"INFO target\")\n",
    "print(target.info(verbose=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "INFO data train ndim  2\nINFO data train shape  (2450, 128)\nINFO data target ndim  1\nINFO data target shape  (2450,)\n"
    }
   ],
   "source": [
    "# Data train into numpy\n",
    "X = np.array(data.values)\n",
    "print(\"INFO data train ndim \", X.ndim)\n",
    "print(\"INFO data train shape \", X.shape)\n",
    "# Data target into numpy\n",
    "y = np.array(target.values)\n",
    "y = le.fit_transform(y)\n",
    "print(\"INFO data target ndim \", y.ndim)\n",
    "print(\"INFO data target shape \", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Bekir indexes  [934, 935, 936, 937, 938, 939, 940, 941, 942, 943]\nBerat indexes  [1716, 1717, 1718, 1719, 1720, 1721, 1722, 1723, 1724, 1725]\nMeyra indexes  [479, 480, 481, 482, 483, 484, 485, 486, 487, 488]\nRamazan indexes  [2249, 2250, 2251, 2252, 2253, 2254, 2255, 2256, 2257, 2258]\nSamet indexes  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\nSamir indexes  [1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394]\n"
    }
   ],
   "source": [
    "# Get indexes of all labels\n",
    "\n",
    "samet_index = data[target[0]==0].index.tolist()\n",
    "meyra_index = data[target[0]==1].index.tolist()\n",
    "bekir_index = data[target[0]==2].index.tolist()\n",
    "samir_index = data[target[0]==3].index.tolist()\n",
    "berat_index = data[target[0]==4].index.tolist()\n",
    "ramazan_index = data[target[0]==5].index.tolist()\n",
    "\n",
    "# Show first 10 indexes\n",
    "print(\"Bekir indexes \", bekir_index[:10])\n",
    "print(\"Berat indexes \", berat_index[:10])\n",
    "print(\"Meyra indexes \", meyra_index[:10])\n",
    "print(\"Ramazan indexes \", ramazan_index[:10])\n",
    "print(\"Samet indexes \", samet_index[:10])\n",
    "print(\"Samir indexes \", samir_index[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[ 934  935  936  937  938  939  940  941 1716 1717 1718 1719 1720 1721\n 1722 1723  479  480  481  482  483  484  485  486 2249 2250 2251 2252\n 2253 2254 2255 2256    0    1    2    3    4    5    6    7 1385 1386\n 1387 1388 1389 1390 1391 1392]\n"
    }
   ],
   "source": [
    "# Get all indexes together\n",
    "all_indexes = np.concatenate([bekir_index[:8],\n",
    "                                berat_index[:8],\n",
    "                                meyra_index[:8],\n",
    "                                ramazan_index[:8],\n",
    "                                samet_index[:8],\n",
    "                                samir_index[:8]])\n",
    "# all_indexes = np.vstack(all_indexes,\n",
    "print(all_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "x_ten            0         1         2         3         4         5         6    \\\n934  -0.141236  0.182262  0.056664  0.152511  0.083062  0.023147 -0.092564   \n935  -0.172426  0.129670 -0.045568  0.080295  0.105105 -0.016185 -0.154171   \n936  -0.128303  0.167794 -0.004384  0.085313  0.171333  0.047267 -0.121825   \n937  -0.137400  0.129731 -0.022547  0.082919  0.111654  0.082956 -0.093362   \n938  -0.110589  0.173985  0.081496  0.163161  0.123259  0.008574 -0.142290   \n939  -0.041491  0.110605  0.031581  0.142755  0.098926  0.053113 -0.075830   \n940  -0.133522  0.133000  0.036692  0.122378  0.095516  0.073730 -0.102879   \n941  -0.091072  0.167259  0.022736  0.111351  0.152868  0.043894 -0.087147   \n1716 -0.151765 -0.011192 -0.142109  0.049688  0.227407  0.167846 -0.020472   \n1717 -0.124634 -0.072246 -0.201699  0.072524  0.216919  0.170886  0.012949   \n1718 -0.105080 -0.089958 -0.210211  0.052916  0.201809  0.093275 -0.049499   \n1719 -0.133819 -0.056217 -0.181862  0.052099  0.196044  0.096663 -0.054693   \n1720 -0.164680 -0.076761 -0.211299  0.062554  0.193957  0.097450 -0.048492   \n1721 -0.143054 -0.110554 -0.230768  0.095720  0.185764  0.114229 -0.055232   \n1722 -0.114938 -0.052798 -0.172038  0.097067  0.220021  0.152659 -0.010682   \n1723 -0.141006 -0.026384 -0.200074  0.030322  0.107728  0.112569 -0.058903   \n479  -0.134761  0.066272  0.105393  0.114516 -0.058807 -0.025190 -0.118554   \n480  -0.070331  0.040868 -0.014257  0.120252  0.005171 -0.057160 -0.168698   \n481  -0.067892  0.067518 -0.037724  0.129943 -0.076609  0.028975 -0.051611   \n482  -0.121357  0.079258 -0.030665  0.123543 -0.031309 -0.015249 -0.102003   \n483  -0.062075  0.044715 -0.038615  0.067312 -0.055843  0.023516 -0.086730   \n484  -0.004511  0.057683 -0.001055  0.103489 -0.045464 -0.073652 -0.076271   \n485   0.066585 -0.002042  0.101067  0.121052 -0.081654 -0.019445 -0.007716   \n486  -0.129877  0.025348 -0.018020  0.108634 -0.005041 -0.055920 -0.145997   \n2249 -0.025031  0.033966  0.051749 -0.005449  0.091905  0.038329  0.074536   \n2250 -0.008802  0.103212  0.114970  0.044765  0.053944  0.036782  0.078081   \n2251 -0.008802  0.103212  0.114970  0.044765  0.053944  0.036782  0.078081   \n2252 -0.007137  0.076246  0.112636  0.033642  0.083548  0.033115  0.052021   \n2253 -0.057258  0.025942  0.039479 -0.012795  0.096206  0.075805  0.078783   \n2254 -0.057258  0.025942  0.039479 -0.012795  0.096206  0.075805  0.078783   \n2255 -0.029942 -0.021808  0.012136  0.009398  0.038041 -0.044011  0.092444   \n2256 -0.005996  0.026577  0.051966 -0.033423  0.083935  0.104371  0.008511   \n0    -0.036097 -0.062722 -0.138305 -0.023945  0.158750  0.115368  0.066765   \n1    -0.007255 -0.055512 -0.150672 -0.044281  0.180425  0.167236  0.050516   \n2    -0.055127 -0.092832 -0.162854  0.002045  0.159195  0.075029  0.070911   \n3    -0.079873 -0.076595 -0.164363  0.000466  0.134778  0.095696 -0.044857   \n4    -0.030315 -0.055627 -0.141346 -0.019156  0.168101  0.065474  0.114286   \n5    -0.090943 -0.084851 -0.143699 -0.022528  0.171669  0.075877  0.071232   \n6    -0.036741 -0.083044 -0.185628 -0.052046  0.151033  0.114224  0.054298   \n7    -0.097186 -0.021058 -0.189680 -0.038473  0.140405  0.073158 -0.040677   \n1385  0.011296  0.085030  0.053378  0.008686 -0.064630  0.101883 -0.195978   \n1386 -0.042391  0.103995  0.048847  0.003138 -0.076258  0.090029 -0.203510   \n1387 -0.025485  0.102357  0.066248  0.009761 -0.063254  0.117716 -0.217594   \n1388 -0.045263  0.039170  0.066365  0.004597 -0.094528  0.101063 -0.135450   \n1389  0.009898  0.055380  0.035460  0.043163 -0.065155  0.108556 -0.198690   \n1390 -0.081198  0.116138  0.045936 -0.022857 -0.053637  0.091898 -0.216672   \n1391  0.045502 -0.003683  0.099556  0.072249 -0.082491  0.093371 -0.184449   \n1392 -0.054983  0.064057  0.046408 -0.000474 -0.116000  0.085190 -0.196891   \n\n           7         8         9    ...       118       119       120  \\\n934  -0.123919  0.036245 -0.059159  ... -0.104611 -0.047626  0.121834   \n935  -0.023634 -0.065791 -0.150818  ... -0.102910  0.000593  0.130436   \n936  -0.080175 -0.024824 -0.121706  ... -0.136008 -0.072596  0.114800   \n937  -0.060607 -0.039211 -0.080643  ... -0.096841  0.008682  0.186951   \n938  -0.105792 -0.046862 -0.099199  ... -0.122794 -0.075878  0.124534   \n939  -0.080881  0.057372 -0.052691  ... -0.089102 -0.012204  0.114531   \n940  -0.099755 -0.063830 -0.105595  ... -0.107758  0.040046  0.053210   \n941  -0.120265 -0.070524 -0.096580  ... -0.106449 -0.023825  0.106828   \n1716  0.000992 -0.067416  0.001675  ...  0.015843 -0.043248  0.094100   \n1717 -0.003067 -0.034211  0.012615  ...  0.030201 -0.031655  0.139529   \n1718 -0.030298 -0.058000  0.031915  ...  0.034888 -0.010725  0.140663   \n1719 -0.022984 -0.074102 -0.013079  ...  0.044588 -0.036455  0.116399   \n1720 -0.007451 -0.042938  0.035795  ...  0.022369  0.001701  0.144478   \n1721 -0.005793 -0.029539  0.039453  ... -0.008435  0.014682  0.111573   \n1722 -0.037653 -0.008614  0.004209  ...  0.012572 -0.041818  0.130245   \n1723 -0.041088 -0.014929 -0.051585  ...  0.074453 -0.037160  0.082777   \n479  -0.136831  0.072510 -0.090896  ... -0.053386 -0.089758  0.080337   \n480  -0.154688 -0.013376 -0.139593  ... -0.090970 -0.054356  0.131555   \n481  -0.159251  0.036604 -0.063258  ... -0.086793 -0.043570  0.153617   \n482  -0.158155  0.062739 -0.074130  ... -0.047619 -0.068439  0.183389   \n483  -0.219204  0.060765 -0.063506  ...  0.000754 -0.050339  0.158834   \n484  -0.188793 -0.024778 -0.055408  ... -0.160470 -0.074535  0.277662   \n485  -0.048719 -0.002461  0.017123  ... -0.070967 -0.146546  0.097680   \n486  -0.110879  0.036627 -0.050492  ... -0.064658 -0.064621  0.158106   \n2249 -0.088523  0.015226  0.052932  ... -0.068847 -0.018829  0.032177   \n2250 -0.112699 -0.036781  0.011135  ... -0.083258 -0.014685  0.067983   \n2251 -0.112699 -0.036781  0.011135  ... -0.083258 -0.014685  0.067983   \n2252 -0.085727 -0.021369  0.040039  ... -0.049931 -0.009536  0.097721   \n2253 -0.045459  0.025983  0.060871  ... -0.082350 -0.011824 -0.000347   \n2254 -0.045459  0.025983  0.060871  ... -0.082350 -0.011824 -0.000347   \n2255 -0.101708 -0.009414  0.006243  ... -0.115943 -0.023348  0.076053   \n2256 -0.055885 -0.101138 -0.020614  ... -0.018505 -0.067245  0.039689   \n0    -0.014943 -0.047392 -0.143377  ... -0.052132 -0.053383  0.010852   \n1    -0.072983 -0.040834 -0.107062  ... -0.066836 -0.014427  0.082855   \n2    -0.010881 -0.078560 -0.133603  ... -0.020766 -0.084379  0.017491   \n3    -0.090109 -0.045213 -0.084140  ... -0.055584 -0.046327  0.008733   \n4    -0.019684 -0.001649 -0.143758  ... -0.026117 -0.039791  0.015301   \n5    -0.005627 -0.076807 -0.130495  ... -0.095965 -0.018726 -0.007270   \n6    -0.002576 -0.007020 -0.083703  ... -0.036458 -0.010540 -0.009035   \n7    -0.076338 -0.016801 -0.087189  ... -0.053812 -0.027023 -0.000889   \n1385  0.015671 -0.202418 -0.160448  ...  0.115527 -0.038034  0.112184   \n1386  0.045964 -0.116441 -0.126658  ...  0.133905 -0.016568  0.084965   \n1387  0.069774 -0.157148 -0.085220  ...  0.123369 -0.034499  0.093638   \n1388  0.099563 -0.121963 -0.052456  ...  0.163519 -0.087845  0.022806   \n1389  0.015911 -0.184493 -0.120757  ...  0.053628 -0.042896  0.069205   \n1390  0.074075 -0.130192 -0.087616  ...  0.115452 -0.045789  0.061476   \n1391  0.057472 -0.167865 -0.074750  ...  0.065257 -0.076379  0.043283   \n1392  0.064992 -0.209126 -0.174044  ...  0.112741 -0.070892  0.065007   \n\n           121       122       123       124       125       126       127  \n934   0.179762 -0.017347  0.008359  0.062670 -0.078373 -0.042598  0.019582  \n935   0.225266 -0.084585 -0.095831  0.043596 -0.005548 -0.031472 -0.026424  \n936   0.159348 -0.034028 -0.076331  0.090079 -0.080794 -0.071002  0.033135  \n937   0.171635 -0.029339 -0.097474  0.033675 -0.037784 -0.020224  0.004597  \n938   0.199194 -0.013245 -0.077795  0.072407 -0.045857 -0.102757 -0.022754  \n939   0.104207 -0.113281 -0.132489  0.016064 -0.025311 -0.066756  0.036115  \n940   0.161246 -0.043947 -0.051046  0.009384 -0.056961 -0.080840  0.009038  \n941   0.189305  0.025780 -0.115816  0.060286 -0.046626 -0.079291 -0.017822  \n1716  0.108066  0.061597 -0.024869 -0.042547 -0.143540 -0.010120  0.010617  \n1717  0.123770  0.008869 -0.041684 -0.069185 -0.131065  0.038842  0.018576  \n1718  0.120142  0.039925 -0.056840 -0.049773 -0.094168  0.028814  0.044170  \n1719  0.102583  0.043362 -0.042621 -0.027003 -0.119942  0.005996  0.041500  \n1720  0.101451  0.044506 -0.028225 -0.016722 -0.130686  0.002998  0.059360  \n1721  0.122225  0.026342 -0.029693 -0.027278 -0.114920  0.063081  0.009032  \n1722  0.133716  0.022856 -0.035439 -0.051431 -0.114706  0.014908 -0.017442  \n1723  0.138181 -0.023889 -0.023231 -0.019323 -0.127636 -0.025325  0.029251  \n479  -0.041748  0.090335  0.079490  0.087041 -0.104528  0.147845 -0.005830  \n480  -0.023530  0.085087  0.053814  0.031182 -0.122679  0.142670  0.028062  \n481   0.095506  0.061518  0.138263 -0.004124 -0.121992  0.140104 -0.057831  \n482   0.023841  0.033709  0.094858 -0.037080 -0.066627  0.127150 -0.093377  \n483   0.069140  0.027472  0.100979  0.020880 -0.101866  0.082393 -0.129908  \n484   0.028357  0.088527  0.136504  0.012978 -0.031560  0.181826 -0.056607  \n485  -0.129224 -0.024843  0.102420  0.004529 -0.138815 -0.058648 -0.118394  \n486  -0.064947  0.147087  0.066469  0.042794 -0.102035  0.162006  0.050235  \n2249  0.058102 -0.041591  0.111199  0.123279 -0.071850 -0.040127 -0.025230  \n2250  0.066260 -0.032890  0.055437  0.144801 -0.052387 -0.030313  0.000824  \n2251  0.066260 -0.032890  0.055437  0.144801 -0.052387 -0.030313  0.000824  \n2252  0.096457 -0.052424  0.043053  0.156550 -0.043684 -0.034680 -0.040420  \n2253  0.046023 -0.018723  0.107897  0.180427 -0.060872 -0.023672 -0.003113  \n2254  0.046023 -0.018723  0.107897  0.180427 -0.060872 -0.023672 -0.003113  \n2255  0.052539 -0.068185  0.081201  0.112583 -0.035979 -0.053197 -0.006565  \n2256  0.116662 -0.010136  0.051305  0.160871 -0.170249 -0.126635  0.015619  \n0    -0.036028 -0.032719  0.059644 -0.030630 -0.017200 -0.060757  0.100916  \n1    -0.055209 -0.033568  0.048271 -0.081447 -0.034860 -0.071171  0.110100  \n2     0.006223 -0.050118  0.048933  0.017041 -0.042428 -0.111494  0.111673  \n3    -0.155385  0.014930  0.113270 -0.052694 -0.035386 -0.121243  0.187436  \n4    -0.008482 -0.060086  0.023200 -0.023407 -0.047063 -0.079421  0.083192  \n5    -0.010116 -0.021352  0.061234 -0.042534 -0.057679 -0.073894  0.096260  \n6    -0.067008  0.086221  0.049132  0.007107 -0.086372 -0.100788  0.156235  \n7    -0.138852  0.020464  0.174709  0.003074 -0.053757 -0.083414  0.202151  \n1385 -0.075733 -0.048191  0.037778  0.083655 -0.065335 -0.048985  0.029475  \n1386 -0.104454 -0.063785  0.051138  0.100228 -0.058918 -0.044693  0.090065  \n1387 -0.095279 -0.048440  0.011371  0.077978  0.016656 -0.016332  0.063293  \n1388 -0.119003 -0.030124  0.100478  0.119466 -0.011479 -0.031744  0.217655  \n1389 -0.064782 -0.024459  0.007147  0.124954 -0.038636 -0.011503  0.069931  \n1390 -0.154865 -0.042209  0.089452  0.076045 -0.008680 -0.048226  0.165829  \n1391 -0.134399 -0.027271  0.031471  0.087447 -0.019418 -0.034423  0.052263  \n1392 -0.057210 -0.084914  0.043889  0.122770 -0.045670 -0.028864  0.060955  \n\n[48 rows x 128 columns]\ny_ten [0 0 0 ... 5 5 5]\n"
    }
   ],
   "source": [
    "# get data and labels of all indexes\n",
    "x_ten = data.loc[ all_indexes, : ]\n",
    "y_ten = target.loc[ all_indexes , : ]\n",
    "\n",
    "print(\"x_ten\", x_ten)\n",
    "print(\"y_ten\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_ten, y_ten, test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train data lenght  28\n# Tuning hyper-parameters for precision\n\nBest parameters set found on development set:\n\n{'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n\nGrid scores on development set:\n\n0.156 (+/-0.430) for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n0.156 (+/-0.430) for {'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'}\n0.156 (+/-0.430) for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n0.156 (+/-0.430) for {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}\n0.156 (+/-0.430) for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n0.156 (+/-0.430) for {'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'}\n1.000 (+/-0.000) for {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n0.156 (+/-0.430) for {'C': 1000, 'gamma': 0.0001, 'kernel': 'rbf'}\n1.000 (+/-0.000) for {'C': 1, 'kernel': 'linear'}\n1.000 (+/-0.000) for {'C': 10, 'kernel': 'linear'}\n1.000 (+/-0.000) for {'C': 100, 'kernel': 'linear'}\n1.000 (+/-0.000) for {'C': 1000, 'kernel': 'linear'}\n0.156 (+/-0.430) for {'degree': 1, 'kernel': 'poly'}\n0.170 (+/-0.478) for {'degree': 3, 'kernel': 'poly'}\n0.170 (+/-0.478) for {'degree': 5, 'kernel': 'poly'}\n0.170 (+/-0.478) for {'degree': 7, 'kernel': 'poly'}\n\nDetailed classification report:\n\nThe model is trained on the full development set.\nThe scores are computed on the full evaluation set.\n\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00         3\n           1       1.00      1.00      1.00         2\n           2       1.00      1.00      1.00         3\n           3       1.00      1.00      1.00         3\n           4       1.00      1.00      1.00         5\n           5       1.00      1.00      1.00         4\n\n    accuracy                           1.00        20\n   macro avg       1.00      1.00      1.00        20\nweighted avg       1.00      1.00      1.00        20\n\n\n# Tuning hyper-parameters for recall\n\nBest parameters set found on development set:\n\n{'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n\nGrid scores on development set:\n\n0.292 (+/-0.433) for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n0.292 (+/-0.433) for {'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'}\n0.292 (+/-0.433) for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n0.292 (+/-0.433) for {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}\n0.292 (+/-0.433) for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n0.292 (+/-0.433) for {'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'}\n1.000 (+/-0.000) for {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n0.292 (+/-0.433) for {'C': 1000, 'gamma': 0.0001, 'kernel': 'rbf'}\n1.000 (+/-0.000) for {'C': 1, 'kernel': 'linear'}\n1.000 (+/-0.000) for {'C': 10, 'kernel': 'linear'}\n1.000 (+/-0.000) for {'C': 100, 'kernel': 'linear'}\n1.000 (+/-0.000) for {'C': 1000, 'kernel': 'linear'}\n0.292 (+/-0.433) for {'degree': 1, 'kernel': 'poly'}\n0.292 (+/-0.433) for {'degree': 3, 'kernel': 'poly'}\n0.292 (+/-0.433) for {'degree': 5, 'kernel': 'poly'}\n0.292 (+/-0.433) for {'degree': 7, 'kernel': 'poly'}\n\nDetailed classification report:\n\nThe model is trained on the full development set.\nThe scores are computed on the full evaluation set.\n\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00         3\n           1       1.00      1.00      1.00         2\n           2       1.00      1.00      1.00         3\n           3       1.00      1.00      1.00         3\n           4       1.00      1.00      1.00         5\n           5       1.00      1.00      1.00         4\n\n    accuracy                           1.00        20\n   macro avg       1.00      1.00      1.00        20\nweighted avg       1.00      1.00      1.00        20\n\n\n/home/samir/.local/lib/python3.6/site-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n  DeprecationWarning)\n/home/samir/.local/lib/python3.6/site-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n  DeprecationWarning)\n"
    }
   ],
   "source": [
    "# Supervised SVC parameter grid search\n",
    "n_samples = len(X_train)\n",
    "print(\"Train data lenght \",n_samples)\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],\n",
    "                     'C': [1, 10, 100, 1000]},\n",
    "                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]},\n",
    "                    {'kernel': ['poly'], 'degree': [1, 3, 5, 7]}]\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(SVC(), tuned_parameters, scoring='%s_macro' % score)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train data lenght  28\nTest data lenght  20\n# Tuning hyper-parameters for precision\n\nBest parameters set found on development set:\n\n{'var_smoothing': 0.1}\n\nGrid scores on development set:\n\n1.000 (+/-0.000) for {'var_smoothing': 0.1}\n0.978 (+/-0.054) for {'var_smoothing': 0.01}\n0.912 (+/-0.171) for {'var_smoothing': 0.001}\n0.681 (+/-0.202) for {'var_smoothing': 0.0001}\n\nDetailed classification report:\n\nThe model is trained on the full development set.\nThe scores are computed on the full evaluation set.\n\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00         3\n           1       1.00      1.00      1.00         2\n           2       1.00      1.00      1.00         3\n           3       1.00      1.00      1.00         3\n           4       1.00      1.00      1.00         5\n           5       1.00      1.00      1.00         4\n\n    accuracy                           1.00        20\n   macro avg       1.00      1.00      1.00        20\nweighted avg       1.00      1.00      1.00        20\n\n\nShape y_true    2252  1385  2251  1388  2249  5     1720  482   938   2250  1716  937   \\\n0     5     3     5     3     5     0     4     1     2     5     4     2   \n\n   940   7     1     1721  480   1390  1723  1717  \n0     2     0     0     4     1     3     4     4  \n\n/home/samir/.local/lib/python3.6/site-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n  DeprecationWarning)\n"
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Unable to coerce to Series, length must be 1: given 20",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-1ef81ffad1cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     print(\"Number of mislabeled points out of a total %d points: %d\"\n\u001b[0;32m---> 45\u001b[0;31m      % (X_test.shape[0], (y_test != y_pred).sum()))\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/ops/__init__.py\u001b[0m in \u001b[0;36mf\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    831\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_align_method_FRAME\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/ops/__init__.py\u001b[0m in \u001b[0;36m_align_method_FRAME\u001b[0;34m(left, right, axis)\u001b[0m\n\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 647\u001b[0;31m             \u001b[0mright\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_series\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/ops/__init__.py\u001b[0m in \u001b[0;36mto_series\u001b[0;34m(right)\u001b[0m\n\u001b[1;32m    637\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m                 raise ValueError(\n\u001b[0;32m--> 639\u001b[0;31m                     \u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgiven_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    640\u001b[0m                 )\n\u001b[1;32m    641\u001b[0m             \u001b[0mright\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor_sliced\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to coerce to Series, length must be 1: given 20"
     ]
    }
   ],
   "source": [
    "# Supervised Normal Bayes parameter grid search\n",
    "\n",
    "n_samples = len(X_train)\n",
    "n_samples_test = len(X_test)\n",
    "print(\"Train data lenght \",n_samples)\n",
    "print(\"Test data lenght \",n_samples_test)\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{ 'var_smoothing': [1e-1, 1e-2, 1e-3, 1e-4]}\n",
    "                    ]\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(GaussianNB(), tuned_parameters, scoring='%s_macro' % score)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()\n",
    "    print(\"Shape y_true {}\" .format(y_true.T))\n",
    "    y_true = y_true.T\n",
    "    print()\n",
    "    print(\"Number of mislabeled points out of a total %d points: %d\"\n",
    "     % (X_test.shape[0], (y_true != y_pred).sum()))\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1595319033080",
   "display_name": "Python 3.7.7 64-bit ('faceNetEmbeddingsClassifier': pipenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}