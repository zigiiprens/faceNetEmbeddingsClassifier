{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import unique\n",
    "from numpy import where\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier, NeighborhoodComponentsAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pipelinehelper.pipelinehelper import PipelineHelper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = pd.read_csv(\"data/data.csv\",  header=None)\n",
    "target = pd.read_csv(\"data/id.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "INFO data\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 2450 entries, 0 to 2449\nData columns (total 128 columns):\n #   Column  Dtype  \n---  ------  -----  \n 0   0       float64\n 1   1       float64\n 2   2       float64\n 3   3       float64\n 4   4       float64\n 5   5       float64\n 6   6       float64\n 7   7       float64\n 8   8       float64\n 9   9       float64\n 10  10      float64\n 11  11      float64\n 12  12      float64\n 13  13      float64\n 14  14      float64\n 15  15      float64\n 16  16      float64\n 17  17      float64\n 18  18      float64\n 19  19      float64\n 20  20      float64\n 21  21      float64\n 22  22      float64\n 23  23      float64\n 24  24      float64\n 25  25      float64\n 26  26      float64\n 27  27      float64\n 28  28      float64\n 29  29      float64\n 30  30      float64\n 31  31      float64\n 32  32      float64\n 33  33      float64\n 34  34      float64\n 35  35      float64\n 36  36      float64\n 37  37      float64\n 38  38      float64\n 39  39      float64\n 40  40      float64\n 41  41      float64\n 42  42      float64\n 43  43      float64\n 44  44      float64\n 45  45      float64\n 46  46      float64\n 47  47      float64\n 48  48      float64\n 49  49      float64\n 50  50      float64\n 51  51      float64\n 52  52      float64\n 53  53      float64\n 54  54      float64\n 55  55      float64\n 56  56      float64\n 57  57      float64\n 58  58      float64\n 59  59      float64\n 60  60      float64\n 61  61      float64\n 62  62      float64\n 63  63      float64\n 64  64      float64\n 65  65      float64\n 66  66      float64\n 67  67      float64\n 68  68      float64\n 69  69      float64\n 70  70      float64\n 71  71      float64\n 72  72      float64\n 73  73      float64\n 74  74      float64\n 75  75      float64\n 76  76      float64\n 77  77      float64\n 78  78      float64\n 79  79      float64\n 80  80      float64\n 81  81      float64\n 82  82      float64\n 83  83      float64\n 84  84      float64\n 85  85      float64\n 86  86      float64\n 87  87      float64\n 88  88      float64\n 89  89      float64\n 90  90      float64\n 91  91      float64\n 92  92      float64\n 93  93      float64\n 94  94      float64\n 95  95      float64\n 96  96      float64\n 97  97      float64\n 98  98      float64\n 99  99      float64\n 100 100     float64\n 101 101     float64\n 102 102     float64\n 103 103     float64\n 104 104     float64\n 105 105     float64\n 106 106     float64\n 107 107     float64\n 108 108     float64\n 109 109     float64\n 110 110     float64\n 111 111     float64\n 112 112     float64\n 113 113     float64\n 114 114     float64\n 115 115     float64\n 116 116     float64\n 117 117     float64\n 118 118     float64\n 119 119     float64\n 120 120     float64\n 121 121     float64\n 122 122     float64\n 123 123     float64\n 124 124     float64\n 125 125     float64\n 126 126     float64\n 127 127     float64\ndtypes: float64(128)\nmemory usage: 2.4 MB\nNone\nINFO target\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 2450 entries, 0 to 2449\nData columns (total 1 columns):\n #   Column  Non-Null Count  Dtype\n---  ------  --------------  -----\n 0   0       2450 non-null   int64\ndtypes: int64(1)\nmemory usage: 19.3 KB\nNone\n"
    }
   ],
   "source": [
    "#print(data.head)\n",
    "print(\"INFO data\")\n",
    "print(data.info(verbose=True))\n",
    "print(\"INFO target\")\n",
    "print(target.info(verbose=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "INFO data train ndim  2\nINFO data train shape  (2450, 128)\nINFO data target ndim  2\nINFO data target shape  (2450, 1)\nINFO data target example [[0]\n [0]\n [0]\n ...\n [5]\n [5]\n [5]]\n"
    }
   ],
   "source": [
    "# Data train into numpy\n",
    "X = np.array(data.values)\n",
    "print(\"INFO data train ndim \", X.ndim)\n",
    "print(\"INFO data train shape \", X.shape)\n",
    "# Data target into numpy\n",
    "y = np.array(target.values)\n",
    "le.fit([0, 1, 2, 3, 4, 5])\n",
    "le.transform(y)\n",
    "print(\"INFO data target ndim \", y.ndim)\n",
    "print(\"INFO data target shape \", y.shape)\n",
    "print(\"INFO data target example\", y[:-10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Bekir indexes  [934, 935, 936, 937, 938, 939, 940, 941, 942, 943]\nBerat indexes  [1716, 1717, 1718, 1719, 1720, 1721, 1722, 1723, 1724, 1725]\nMeyra indexes  [479, 480, 481, 482, 483, 484, 485, 486, 487, 488]\nRamazan indexes  [2249, 2250, 2251, 2252, 2253, 2254, 2255, 2256, 2257, 2258]\nSamet indexes  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\nSamir indexes  [1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394]\n"
    }
   ],
   "source": [
    "# Get indexes of all labels\n",
    "\n",
    "samet_index = data[target[0]==0].index.tolist()\n",
    "meyra_index = data[target[0]==1].index.tolist()\n",
    "bekir_index = data[target[0]==2].index.tolist()\n",
    "samir_index = data[target[0]==3].index.tolist()\n",
    "berat_index = data[target[0]==4].index.tolist()\n",
    "ramazan_index = data[target[0]==5].index.tolist()\n",
    "\n",
    "# Show first 10 indexes\n",
    "print(\"Bekir indexes \", bekir_index[:10])\n",
    "print(\"Berat indexes \", berat_index[:10])\n",
    "print(\"Meyra indexes \", meyra_index[:10])\n",
    "print(\"Ramazan indexes \", ramazan_index[:10])\n",
    "print(\"Samet indexes \", samet_index[:10])\n",
    "print(\"Samir indexes \", samir_index[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[ 934  935  936  937  938  939  940  941  942  943  944  945  946  947\n  948  949  950  951  952  953  954  955  956  957  958  959  960  961\n  962  963  964  965  966  967  968  969  970  971  972  973  974  975\n  976  977  978  979  980  981  982  983 1716 1717 1718 1719 1720 1721\n 1722 1723 1724 1725 1726 1727 1728 1729 1730 1731 1732 1733 1734 1735\n 1736 1737 1738 1739 1740 1741 1742 1743 1744 1745 1746 1747 1748 1749\n 1750 1751 1752 1753 1754 1755 1756 1757 1758 1759 1760 1761 1762 1763\n 1764 1765  479  480  481  482  483  484  485  486  487  488  489  490\n  491  492  493  494  495  496  497  498  499  500  501  502  503  504\n  505  506  507  508  509  510  511  512  513  514  515  516  517  518\n  519  520  521  522  523  524  525  526  527  528 2249 2250 2251 2252\n 2253 2254 2255 2256 2257 2258 2259 2260 2261 2262 2263 2264 2265 2266\n 2267 2268 2269 2270 2271 2272 2273 2274 2275 2276 2277 2278 2279 2280\n 2281 2282 2283 2284 2285 2286 2287 2288 2289 2290 2291 2292 2293 2294\n 2295 2296 2297 2298    0    1    2    3    4    5    6    7    8    9\n   10   11   12   13   14   15   16   17   18   19   20   21   22   23\n   24   25   26   27   28   29   30   31   32   33   34   35   36   37\n   38   39   40   41   42   43   44   45   46   47   48   49 1385 1386\n 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400\n 1401 1402 1403 1404 1405 1406 1407 1408 1409 1410 1411 1412 1413 1414\n 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428\n 1429 1430 1431 1432 1433 1434]\n"
    }
   ],
   "source": [
    "# Get all indexes together\n",
    "all_indexes = np.concatenate([bekir_index[:50],\n",
    "                                berat_index[:50],\n",
    "                                meyra_index[:50],\n",
    "                                ramazan_index[:50],\n",
    "                                samet_index[:50],\n",
    "                                samir_index[:50]])\n",
    "# all_indexes = np.vstack(all_indexes,\n",
    "print(all_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "x_ten            0         1         2         3         4         5         6    \\\n934  -0.141236  0.182262  0.056664  0.152511  0.083062  0.023147 -0.092564   \n935  -0.172426  0.129670 -0.045568  0.080295  0.105105 -0.016185 -0.154171   \n936  -0.128303  0.167794 -0.004384  0.085313  0.171333  0.047267 -0.121825   \n937  -0.137400  0.129731 -0.022547  0.082919  0.111654  0.082956 -0.093362   \n938  -0.110589  0.173985  0.081496  0.163161  0.123259  0.008574 -0.142290   \n...        ...       ...       ...       ...       ...       ...       ...   \n1430  0.000243  0.055220  0.035373  0.001815 -0.046349  0.119317 -0.226786   \n1431 -0.036954  0.077120  0.121743 -0.033962 -0.057352  0.107758 -0.174992   \n1432 -0.028559  0.048405 -0.016701  0.027891 -0.108294  0.097568 -0.193534   \n1433 -0.026298  0.071113  0.031832 -0.020578 -0.049751  0.078993 -0.188323   \n1434 -0.028949  0.065760  0.042407  0.012907 -0.080943  0.081421 -0.170090   \n\n           7         8         9    ...       118       119       120  \\\n934  -0.123919  0.036245 -0.059159  ... -0.104611 -0.047626  0.121834   \n935  -0.023634 -0.065791 -0.150818  ... -0.102910  0.000593  0.130436   \n936  -0.080175 -0.024824 -0.121706  ... -0.136008 -0.072596  0.114800   \n937  -0.060607 -0.039211 -0.080643  ... -0.096841  0.008682  0.186951   \n938  -0.105792 -0.046862 -0.099199  ... -0.122794 -0.075878  0.124534   \n...        ...       ...       ...  ...       ...       ...       ...   \n1430  0.050824 -0.176372 -0.125115  ...  0.097249 -0.054494  0.086217   \n1431  0.089504 -0.139943 -0.098411  ...  0.192428 -0.084504  0.067758   \n1432  0.083108 -0.229856 -0.099211  ...  0.083115 -0.067622  0.059661   \n1433  0.072617 -0.165349 -0.092504  ...  0.113839 -0.043096  0.098289   \n1434  0.083407 -0.158173 -0.107404  ...  0.041247 -0.021817  0.100424   \n\n           121       122       123       124       125       126       127  \n934   0.179762 -0.017347  0.008359  0.062670 -0.078373 -0.042598  0.019582  \n935   0.225266 -0.084585 -0.095831  0.043596 -0.005548 -0.031472 -0.026424  \n936   0.159348 -0.034028 -0.076331  0.090079 -0.080794 -0.071002  0.033135  \n937   0.171635 -0.029339 -0.097474  0.033675 -0.037784 -0.020224  0.004597  \n938   0.199194 -0.013245 -0.077795  0.072407 -0.045857 -0.102757 -0.022754  \n...        ...       ...       ...       ...       ...       ...       ...  \n1430 -0.096726 -0.068196  0.026990  0.086984  0.003188 -0.059230  0.085487  \n1431 -0.136812 -0.050319  0.129382  0.064082 -0.075827 -0.100973  0.161432  \n1432 -0.076392 -0.040625  0.045289  0.109680 -0.017819 -0.014732  0.112397  \n1433 -0.138946 -0.071224  0.028635  0.137244 -0.010424 -0.066818  0.095422  \n1434 -0.133177 -0.119605  0.012973  0.024226 -0.048714 -0.009426  0.036566  \n\n[300 rows x 128 columns]\ny_ten [0 0 0 ... 5 5 5]\n"
    }
   ],
   "source": [
    "# get data and labels of all indexes\n",
    "x_ten = data.loc[ all_indexes, : ]\n",
    "y_ten = target.loc[ all_indexes , : ]\n",
    "\n",
    "print(\"x_ten\", x_ten)\n",
    "print(\"y_ten\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_ten, y_ten, test_size=0.6, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train data lenght  180\n# Tuning hyper-parameters for precision\n\nBest parameters set found on development set:\n\n{'C': 10, 'kernel': 'linear'}\n\nGrid scores on development set:\n\n0.032 (+/-0.000) for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n0.032 (+/-0.000) for {'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'}\n0.032 (+/-0.000) for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n0.032 (+/-0.000) for {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}\n0.982 (+/-0.018) for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n0.032 (+/-0.000) for {'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'}\n0.982 (+/-0.018) for {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n0.982 (+/-0.018) for {'C': 1000, 'gamma': 0.0001, 'kernel': 'rbf'}\n0.982 (+/-0.018) for {'C': 1, 'kernel': 'linear'}\n0.986 (+/-0.023) for {'C': 10, 'kernel': 'linear'}\n0.986 (+/-0.023) for {'C': 100, 'kernel': 'linear'}\n0.986 (+/-0.023) for {'C': 1000, 'kernel': 'linear'}\n0.982 (+/-0.018) for {'degree': 1, 'kernel': 'poly'}\n0.982 (+/-0.019) for {'degree': 3, 'kernel': 'poly'}\n0.981 (+/-0.019) for {'degree': 5, 'kernel': 'poly'}\n0.981 (+/-0.019) for {'degree': 7, 'kernel': 'poly'}\n\nDetailed classification report:\n\nThe model is trained on the full development set.\nThe scores are computed on the full evaluation set.\n\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        22\n           1       1.00      1.00      1.00        19\n           2       1.00      0.95      0.97        20\n           3       1.00      1.00      1.00        15\n           4       0.96      1.00      0.98        24\n           5       1.00      1.00      1.00        20\n\n    accuracy                           0.99       120\n   macro avg       0.99      0.99      0.99       120\nweighted avg       0.99      0.99      0.99       120\n\n\n# Tuning hyper-parameters for recall\n\nBest parameters set found on development set:\n\n{'C': 10, 'kernel': 'linear'}\n\nGrid scores on development set:\n\n0.167 (+/-0.000) for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n0.167 (+/-0.000) for {'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'}\n0.167 (+/-0.000) for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n0.167 (+/-0.000) for {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}\n0.980 (+/-0.020) for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n0.167 (+/-0.000) for {'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'}\n0.980 (+/-0.020) for {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n0.980 (+/-0.020) for {'C': 1000, 'gamma': 0.0001, 'kernel': 'rbf'}\n0.980 (+/-0.020) for {'C': 1, 'kernel': 'linear'}\n0.985 (+/-0.025) for {'C': 10, 'kernel': 'linear'}\n0.985 (+/-0.025) for {'C': 100, 'kernel': 'linear'}\n0.985 (+/-0.025) for {'C': 1000, 'kernel': 'linear'}\n0.980 (+/-0.020) for {'degree': 1, 'kernel': 'poly'}\n0.980 (+/-0.020) for {'degree': 3, 'kernel': 'poly'}\n0.980 (+/-0.020) for {'degree': 5, 'kernel': 'poly'}\n0.980 (+/-0.020) for {'degree': 7, 'kernel': 'poly'}\n\nDetailed classification report:\n\nThe model is trained on the full development set.\nThe scores are computed on the full evaluation set.\n\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        22\n           1       1.00      1.00      1.00        19\n           2       1.00      0.95      0.97        20\n           3       1.00      1.00      1.00        15\n           4       0.96      1.00      0.98        24\n           5       1.00      1.00      1.00        20\n\n    accuracy                           0.99       120\n   macro avg       0.99      0.99      0.99       120\nweighted avg       0.99      0.99      0.99       120\n\n\n"
    }
   ],
   "source": [
    "# Supervised SVC parameter grid search\n",
    "n_samples = len(X_train)\n",
    "print(\"Train data lenght \",n_samples)\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],\n",
    "                     'C': [1, 10, 100, 1000]},\n",
    "                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]},\n",
    "                    {'kernel': ['poly'], 'degree': [1, 3, 5, 7]}]\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(SVC(), tuned_parameters, scoring='%s_macro' % score)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data\n",
    "scale_norm = preprocessing.StandardScaler(with_mean=True).fit(X_train)\n",
    "X_train_norm = scale_norm.transform(X_train)\n",
    "X_test_norm = scale_norm.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train data lenght  180\nTest data lenght  120\n# Tuning hyper-parameters for precision\n\nBest parameters set found on development set:\n\n{'var_smoothing': 100.0}\n\nGrid scores on development set:\n\n0.992 (+/-0.020) for {'var_smoothing': 100.0}\n0.992 (+/-0.020) for {'var_smoothing': 10.0}\n0.982 (+/-0.018) for {'var_smoothing': 1.0}\n0.982 (+/-0.019) for {'var_smoothing': 0.1}\n0.982 (+/-0.019) for {'var_smoothing': 0.01}\n0.982 (+/-0.019) for {'var_smoothing': 0.001}\n0.982 (+/-0.019) for {'var_smoothing': 0.0001}\n\nDetailed classification report:\n\nThe model is trained on the full development set.\nThe scores are computed on the full evaluation set.\n\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        22\n           1       1.00      1.00      1.00        19\n           2       1.00      0.95      0.97        20\n           3       1.00      1.00      1.00        15\n           4       0.96      1.00      0.98        24\n           5       1.00      1.00      1.00        20\n\n    accuracy                           0.99       120\n   macro avg       0.99      0.99      0.99       120\nweighted avg       0.99      0.99      0.99       120\n\n\nShape y_true [0 3 5 2 0 0 5 1 2 5 0 4 0 2 5 0 3 0 1 5 4 1 0 3 1 2 3 3 5 0 2 5 2 2 0 1 2\n 4 2 4 4 4 3 4 4 4 0 0 1 0 5 1 0 4 3 3 4 4 5 2 0 4 2 5 5 2 2 3 4 5 1 1 0 5\n 1 3 4 4 4 3 5 5 2 1 4 1 1 4 2 2 5 1 4 0 2 2 5 1 3 4 0 4 2 5 3 0 1 2 5 4 1\n 3 4 0 0 1 1 3 5 0]\nShape y_pred [0 3 5 2 0 0 5 1 2 5 0 4 0 2 5 0 3 0 1 5 4 1 0 3 1 2 3 3 5 0 2 5 2 4 0 1 2\n 4 2 4 4 4 3 4 4 4 0 0 1 0 5 1 0 4 3 3 4 4 5 2 0 4 2 5 5 2 2 3 4 5 1 1 0 5\n 1 3 4 4 4 3 5 5 2 1 4 1 1 4 2 2 5 1 4 0 2 2 5 1 3 4 0 4 2 5 3 0 1 2 5 4 1\n 3 4 0 0 1 1 3 5 0]\n\nNumber of mislabeled points out of a total 120 points: 1\n\n\n# Tuning hyper-parameters for recall\n\nBest parameters set found on development set:\n\n{'var_smoothing': 100.0}\n\nGrid scores on development set:\n\n0.990 (+/-0.025) for {'var_smoothing': 100.0}\n0.990 (+/-0.025) for {'var_smoothing': 10.0}\n0.980 (+/-0.020) for {'var_smoothing': 1.0}\n0.980 (+/-0.020) for {'var_smoothing': 0.1}\n0.980 (+/-0.020) for {'var_smoothing': 0.01}\n0.980 (+/-0.020) for {'var_smoothing': 0.001}\n0.980 (+/-0.020) for {'var_smoothing': 0.0001}\n\nDetailed classification report:\n\nThe model is trained on the full development set.\nThe scores are computed on the full evaluation set.\n\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        22\n           1       1.00      1.00      1.00        19\n           2       1.00      0.95      0.97        20\n           3       1.00      1.00      1.00        15\n           4       0.96      1.00      0.98        24\n           5       1.00      1.00      1.00        20\n\n    accuracy                           0.99       120\n   macro avg       0.99      0.99      0.99       120\nweighted avg       0.99      0.99      0.99       120\n\n\nShape y_true [0 3 5 2 0 0 5 1 2 5 0 4 0 2 5 0 3 0 1 5 4 1 0 3 1 2 3 3 5 0 2 5 2 2 0 1 2\n 4 2 4 4 4 3 4 4 4 0 0 1 0 5 1 0 4 3 3 4 4 5 2 0 4 2 5 5 2 2 3 4 5 1 1 0 5\n 1 3 4 4 4 3 5 5 2 1 4 1 1 4 2 2 5 1 4 0 2 2 5 1 3 4 0 4 2 5 3 0 1 2 5 4 1\n 3 4 0 0 1 1 3 5 0]\nShape y_pred [0 3 5 2 0 0 5 1 2 5 0 4 0 2 5 0 3 0 1 5 4 1 0 3 1 2 3 3 5 0 2 5 2 4 0 1 2\n 4 2 4 4 4 3 4 4 4 0 0 1 0 5 1 0 4 3 3 4 4 5 2 0 4 2 5 5 2 2 3 4 5 1 1 0 5\n 1 3 4 4 4 3 5 5 2 1 4 1 1 4 2 2 5 1 4 0 2 2 5 1 3 4 0 4 2 5 3 0 1 2 5 4 1\n 3 4 0 0 1 1 3 5 0]\n\nNumber of mislabeled points out of a total 120 points: 1\n\n\n"
    }
   ],
   "source": [
    "# Supervised Normal Bayes parameter grid search\n",
    "\n",
    "n_samples = len(X_train_norm)\n",
    "n_samples_test = len(X_test_norm)\n",
    "print(\"Train data lenght \",n_samples)\n",
    "print(\"Test data lenght \",n_samples_test)\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{ 'var_smoothing': [1000e-1, 100e-1, 10e-1, 1e-1, 1e-2, 1e-3, 1e-4]}\n",
    "                    ]\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(GaussianNB(), tuned_parameters, scoring='%s_macro' % score)\n",
    "    clf.fit(X_train_norm, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, clf.predict(X_test_norm)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()\n",
    "    y_true = np.array(y_true).flatten()\n",
    "    print(\"Shape y_true {}\" .format(y_true))\n",
    "    print(\"Shape y_pred {}\" .format(y_pred))\n",
    "    print()\n",
    "    print(\"Number of mislabeled points out of a total %d points: %d\"\n",
    "     % (X_test.shape[0], (y_true != y_pred).sum()))\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train data lenght  120\nTest data lenght  180\n# Tuning hyper-parameters for precision\n\nBest parameters set found on development set:\n\n{'algorithm': 'brute', 'n_neighbors': 6, 'p': 1}\n\nGrid scores on development set:\n\n0.987 (+/-0.018) for {'algorithm': 'brute', 'n_neighbors': 6, 'p': 1}\n0.985 (+/-0.020) for {'algorithm': 'brute', 'n_neighbors': 6, 'p': 2}\n0.987 (+/-0.018) for {'algorithm': 'kd_tree', 'n_neighbors': 6, 'p': 1}\n0.985 (+/-0.020) for {'algorithm': 'kd_tree', 'n_neighbors': 6, 'p': 2}\n\nDetailed classification report:\n\nThe model is trained on the full development set.\nThe scores are computed on the full evaluation set.\n\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        30\n           1       0.94      0.97      0.95        32\n           2       1.00      0.96      0.98        28\n           3       1.00      0.92      0.96        25\n           4       0.97      1.00      0.98        32\n           5       0.97      1.00      0.99        33\n\n    accuracy                           0.98       180\n   macro avg       0.98      0.98      0.98       180\nweighted avg       0.98      0.98      0.98       180\n\n\nShape y_true [0 3 5 2 0 0 5 1 2 5 0 4 0 2 5 0 3 0 1 5 4 1 0 3 1 2 3 3 5 0 2 5 2 2 0 1 2\n 4 2 4 4 4 3 4 4 4 0 0 1 0 5 1 0 4 3 3 4 4 5 2 0 4 2 5 5 2 2 3 4 5 1 1 0 5\n 1 3 4 4 4 3 5 5 2 1 4 1 1 4 2 2 5 1 4 0 2 2 5 1 3 4 0 4 2 5 3 0 1 2 5 4 1\n 3 4 0 0 1 1 3 5 0 4 5 5 5 1 3 2 1 5 3 1 1 5 1 3 3 5 5 5 2 1 1 5 1 0 3 1 4\n 2 4 3 1 0 5 3 0 0 4 5 4 0 5 2 2 2 1 3 3 5 4 4 0 0 2 4 0 1 2 3 1]\nShape y_pred [0 3 5 2 0 0 5 1 2 5 0 4 0 2 5 0 3 0 1 5 4 1 0 3 1 2 3 3 5 0 2 5 2 4 0 1 2\n 4 2 4 4 4 3 4 4 4 0 0 1 0 5 1 0 4 3 3 4 4 5 2 0 4 2 5 5 2 2 3 4 5 1 1 0 5\n 1 1 4 4 4 3 5 5 2 1 4 1 1 4 2 2 5 1 4 0 2 2 5 1 3 4 0 4 2 5 3 0 1 2 5 4 1\n 3 4 0 0 1 1 3 5 0 4 5 5 5 1 3 2 1 5 3 1 1 5 1 3 3 5 5 5 2 1 1 5 1 0 1 1 4\n 2 4 3 1 0 5 3 0 0 4 5 4 0 5 2 2 2 1 3 3 5 4 4 0 0 2 4 0 5 2 3 1]\n\nNumber of mislabeled points out of a total 180 points: 4\n\n\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        30\n           1       0.94      0.97      0.95        32\n           2       1.00      0.96      0.98        28\n           3       1.00      0.92      0.96        25\n           4       0.97      1.00      0.98        32\n           5       0.97      1.00      0.99        33\n\n    accuracy                           0.98       180\n   macro avg       0.98      0.98      0.98       180\nweighted avg       0.98      0.98      0.98       180\n\n\nLabel y_true [0 3 5 2 0 0 5 1 2 5 0 4 0 2 5 0 3 0 1 5 4 1 0 3 1 2 3 3 5 0 2 5 2 2 0 1 2\n 4 2 4 4 4 3 4 4 4 0 0 1 0 5 1 0 4 3 3 4 4 5 2 0 4 2 5 5 2 2 3 4 5 1 1 0 5\n 1 3 4 4 4 3 5 5 2 1 4 1 1 4 2 2 5 1 4 0 2 2 5 1 3 4 0 4 2 5 3 0 1 2 5 4 1\n 3 4 0 0 1 1 3 5 0 4 5 5 5 1 3 2 1 5 3 1 1 5 1 3 3 5 5 5 2 1 1 5 1 0 3 1 4\n 2 4 3 1 0 5 3 0 0 4 5 4 0 5 2 2 2 1 3 3 5 4 4 0 0 2 4 0 1 2 3 1]\n/home/samir/.local/lib/python3.6/site-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n  DeprecationWarning)\n"
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "Required argument 'shape' (pos 2) not found",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-c8735d22fd39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Label y_true {}\"\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Label y_predict_proba {}\"\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munravel_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred_proba\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Label y_pred_proba {}\"\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred_proba\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Required argument 'shape' (pos 2) not found"
     ]
    }
   ],
   "source": [
    "# Supervised KNN parameter grid search\n",
    "\n",
    "n_samples = len(X_train)\n",
    "n_samples_test = len(X_test)\n",
    "print(\"Train data lenght \",n_samples)\n",
    "print(\"Test data lenght \",n_samples_test)\n",
    "\n",
    "# Set classifier by cross-validation\n",
    "pipe = Pipeline([\n",
    "    ('classifier', PipelineHelper([\n",
    "        ('knn', KNeighborsClassifier()),\n",
    "    ])),\n",
    "])\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{ 'n_neighbors': [6],\n",
    "                    'algorithm': ['brute', 'kd_tree'],\n",
    "                    'p': [1,2]}\n",
    "                    ]\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(KNeighborsClassifier(), tuned_parameters, scoring='%s_macro' % score)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()\n",
    "    y_true = np.array(y_true).flatten()\n",
    "    print(\"Shape y_true {}\" .format(y_true))\n",
    "    print(\"Shape y_pred {}\" .format(y_pred))\n",
    "    print()\n",
    "    print(\"Number of mislabeled points out of a total %d points: %d\"\n",
    "     % (X_test.shape[0], (y_true != y_pred).sum()))\n",
    "    print()\n",
    "    print()\n",
    "    y_true, y_pred_proba = y_test, clf.predict_proba(X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()\n",
    "    y_true = np.array(y_true).flatten()\n",
    "    print(\"Label y_true {}\" .format(y_true))\n",
    "    print(\"Label y_predict_proba {}\" .format(np.unravel_index(np.argmax(y_pred_proba, axis=-1),y_pred_proba.shape)))\n",
    "    print(\"Label y_pred_proba {}\" .format(le.inverse_transform(y_pred_proba)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1595407870873",
   "display_name": "Python 3.7.7 64-bit ('faceNetEmbeddingsClassifier': pipenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}