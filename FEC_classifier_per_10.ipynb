{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import unique\n",
    "from numpy import where\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = pd.read_csv(\"data/data.csv\",  header=None)\n",
    "target = pd.read_csv(\"data/id.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "INFO data\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 2450 entries, 0 to 2449\nData columns (total 128 columns):\n #   Column  Dtype  \n---  ------  -----  \n 0   0       float64\n 1   1       float64\n 2   2       float64\n 3   3       float64\n 4   4       float64\n 5   5       float64\n 6   6       float64\n 7   7       float64\n 8   8       float64\n 9   9       float64\n 10  10      float64\n 11  11      float64\n 12  12      float64\n 13  13      float64\n 14  14      float64\n 15  15      float64\n 16  16      float64\n 17  17      float64\n 18  18      float64\n 19  19      float64\n 20  20      float64\n 21  21      float64\n 22  22      float64\n 23  23      float64\n 24  24      float64\n 25  25      float64\n 26  26      float64\n 27  27      float64\n 28  28      float64\n 29  29      float64\n 30  30      float64\n 31  31      float64\n 32  32      float64\n 33  33      float64\n 34  34      float64\n 35  35      float64\n 36  36      float64\n 37  37      float64\n 38  38      float64\n 39  39      float64\n 40  40      float64\n 41  41      float64\n 42  42      float64\n 43  43      float64\n 44  44      float64\n 45  45      float64\n 46  46      float64\n 47  47      float64\n 48  48      float64\n 49  49      float64\n 50  50      float64\n 51  51      float64\n 52  52      float64\n 53  53      float64\n 54  54      float64\n 55  55      float64\n 56  56      float64\n 57  57      float64\n 58  58      float64\n 59  59      float64\n 60  60      float64\n 61  61      float64\n 62  62      float64\n 63  63      float64\n 64  64      float64\n 65  65      float64\n 66  66      float64\n 67  67      float64\n 68  68      float64\n 69  69      float64\n 70  70      float64\n 71  71      float64\n 72  72      float64\n 73  73      float64\n 74  74      float64\n 75  75      float64\n 76  76      float64\n 77  77      float64\n 78  78      float64\n 79  79      float64\n 80  80      float64\n 81  81      float64\n 82  82      float64\n 83  83      float64\n 84  84      float64\n 85  85      float64\n 86  86      float64\n 87  87      float64\n 88  88      float64\n 89  89      float64\n 90  90      float64\n 91  91      float64\n 92  92      float64\n 93  93      float64\n 94  94      float64\n 95  95      float64\n 96  96      float64\n 97  97      float64\n 98  98      float64\n 99  99      float64\n 100 100     float64\n 101 101     float64\n 102 102     float64\n 103 103     float64\n 104 104     float64\n 105 105     float64\n 106 106     float64\n 107 107     float64\n 108 108     float64\n 109 109     float64\n 110 110     float64\n 111 111     float64\n 112 112     float64\n 113 113     float64\n 114 114     float64\n 115 115     float64\n 116 116     float64\n 117 117     float64\n 118 118     float64\n 119 119     float64\n 120 120     float64\n 121 121     float64\n 122 122     float64\n 123 123     float64\n 124 124     float64\n 125 125     float64\n 126 126     float64\n 127 127     float64\ndtypes: float64(128)\nmemory usage: 2.4 MB\nNone\nINFO target\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 2450 entries, 0 to 2449\nData columns (total 1 columns):\n #   Column  Non-Null Count  Dtype\n---  ------  --------------  -----\n 0   0       2450 non-null   int64\ndtypes: int64(1)\nmemory usage: 19.3 KB\nNone\n"
    }
   ],
   "source": [
    "#print(data.head)\n",
    "print(\"INFO data\")\n",
    "print(data.info(verbose=True))\n",
    "print(\"INFO target\")\n",
    "print(target.info(verbose=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "INFO data train ndim  2\nINFO data train shape  (2450, 128)\nINFO data target ndim  1\nINFO data target shape  (2450,)\n"
    }
   ],
   "source": [
    "# Data train into numpy\n",
    "X = np.array(data.values)\n",
    "print(\"INFO data train ndim \", X.ndim)\n",
    "print(\"INFO data train shape \", X.shape)\n",
    "# Data target into numpy\n",
    "y = np.array(target.values)\n",
    "y = le.fit_transform(y)\n",
    "print(\"INFO data target ndim \", y.ndim)\n",
    "print(\"INFO data target shape \", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Bekir indexes  [934, 935, 936, 937, 938, 939, 940, 941, 942, 943]\nBerat indexes  [1716, 1717, 1718, 1719, 1720, 1721, 1722, 1723, 1724, 1725]\nMeyra indexes  [479, 480, 481, 482, 483, 484, 485, 486, 487, 488]\nRamazan indexes  [2249, 2250, 2251, 2252, 2253, 2254, 2255, 2256, 2257, 2258]\nSamet indexes  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\nSamir indexes  [1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394]\n"
    }
   ],
   "source": [
    "# Get indexes of all labels\n",
    "\n",
    "samet_index = data[target[0]==0].index.tolist()\n",
    "meyra_index = data[target[0]==1].index.tolist()\n",
    "bekir_index = data[target[0]==2].index.tolist()\n",
    "samir_index = data[target[0]==3].index.tolist()\n",
    "berat_index = data[target[0]==4].index.tolist()\n",
    "ramazan_index = data[target[0]==5].index.tolist()\n",
    "\n",
    "# Show first 10 indexes\n",
    "print(\"Bekir indexes \", bekir_index[:10])\n",
    "print(\"Berat indexes \", berat_index[:10])\n",
    "print(\"Meyra indexes \", meyra_index[:10])\n",
    "print(\"Ramazan indexes \", ramazan_index[:10])\n",
    "print(\"Samet indexes \", samet_index[:10])\n",
    "print(\"Samir indexes \", samir_index[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[ 934  935  936  937  938  939  940  941 1716 1717 1718 1719 1720 1721\n 1722 1723  479  480  481  482  483  484  485  486 2249 2250 2251 2252\n 2253 2254 2255 2256    0    1    2    3    4    5    6    7 1385 1386\n 1387 1388 1389 1390 1391 1392]\n"
    }
   ],
   "source": [
    "# Get all indexes together\n",
    "all_indexes = np.concatenate([bekir_index[:8],\n",
    "                                berat_index[:8],\n",
    "                                meyra_index[:8],\n",
    "                                ramazan_index[:8],\n",
    "                                samet_index[:8],\n",
    "                                samir_index[:8]])\n",
    "# all_indexes = np.vstack(all_indexes,\n",
    "print(all_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "x_ten            0         1         2         3         4         5         6    \\\n934  -0.141236  0.182262  0.056664  0.152511  0.083062  0.023147 -0.092564   \n935  -0.172426  0.129670 -0.045568  0.080295  0.105105 -0.016185 -0.154171   \n936  -0.128303  0.167794 -0.004384  0.085313  0.171333  0.047267 -0.121825   \n937  -0.137400  0.129731 -0.022547  0.082919  0.111654  0.082956 -0.093362   \n938  -0.110589  0.173985  0.081496  0.163161  0.123259  0.008574 -0.142290   \n939  -0.041491  0.110605  0.031581  0.142755  0.098926  0.053113 -0.075830   \n940  -0.133522  0.133000  0.036692  0.122378  0.095516  0.073730 -0.102879   \n941  -0.091072  0.167259  0.022736  0.111351  0.152868  0.043894 -0.087147   \n1716 -0.151765 -0.011192 -0.142109  0.049688  0.227407  0.167846 -0.020472   \n1717 -0.124634 -0.072246 -0.201699  0.072524  0.216919  0.170886  0.012949   \n1718 -0.105080 -0.089958 -0.210211  0.052916  0.201809  0.093275 -0.049499   \n1719 -0.133819 -0.056217 -0.181862  0.052099  0.196044  0.096663 -0.054693   \n1720 -0.164680 -0.076761 -0.211299  0.062554  0.193957  0.097450 -0.048492   \n1721 -0.143054 -0.110554 -0.230768  0.095720  0.185764  0.114229 -0.055232   \n1722 -0.114938 -0.052798 -0.172038  0.097067  0.220021  0.152659 -0.010682   \n1723 -0.141006 -0.026384 -0.200074  0.030322  0.107728  0.112569 -0.058903   \n479  -0.134761  0.066272  0.105393  0.114516 -0.058807 -0.025190 -0.118554   \n480  -0.070331  0.040868 -0.014257  0.120252  0.005171 -0.057160 -0.168698   \n481  -0.067892  0.067518 -0.037724  0.129943 -0.076609  0.028975 -0.051611   \n482  -0.121357  0.079258 -0.030665  0.123543 -0.031309 -0.015249 -0.102003   \n483  -0.062075  0.044715 -0.038615  0.067312 -0.055843  0.023516 -0.086730   \n484  -0.004511  0.057683 -0.001055  0.103489 -0.045464 -0.073652 -0.076271   \n485   0.066585 -0.002042  0.101067  0.121052 -0.081654 -0.019445 -0.007716   \n486  -0.129877  0.025348 -0.018020  0.108634 -0.005041 -0.055920 -0.145997   \n2249 -0.025031  0.033966  0.051749 -0.005449  0.091905  0.038329  0.074536   \n2250 -0.008802  0.103212  0.114970  0.044765  0.053944  0.036782  0.078081   \n2251 -0.008802  0.103212  0.114970  0.044765  0.053944  0.036782  0.078081   \n2252 -0.007137  0.076246  0.112636  0.033642  0.083548  0.033115  0.052021   \n2253 -0.057258  0.025942  0.039479 -0.012795  0.096206  0.075805  0.078783   \n2254 -0.057258  0.025942  0.039479 -0.012795  0.096206  0.075805  0.078783   \n2255 -0.029942 -0.021808  0.012136  0.009398  0.038041 -0.044011  0.092444   \n2256 -0.005996  0.026577  0.051966 -0.033423  0.083935  0.104371  0.008511   \n0    -0.036097 -0.062722 -0.138305 -0.023945  0.158750  0.115368  0.066765   \n1    -0.007255 -0.055512 -0.150672 -0.044281  0.180425  0.167236  0.050516   \n2    -0.055127 -0.092832 -0.162854  0.002045  0.159195  0.075029  0.070911   \n3    -0.079873 -0.076595 -0.164363  0.000466  0.134778  0.095696 -0.044857   \n4    -0.030315 -0.055627 -0.141346 -0.019156  0.168101  0.065474  0.114286   \n5    -0.090943 -0.084851 -0.143699 -0.022528  0.171669  0.075877  0.071232   \n6    -0.036741 -0.083044 -0.185628 -0.052046  0.151033  0.114224  0.054298   \n7    -0.097186 -0.021058 -0.189680 -0.038473  0.140405  0.073158 -0.040677   \n1385  0.011296  0.085030  0.053378  0.008686 -0.064630  0.101883 -0.195978   \n1386 -0.042391  0.103995  0.048847  0.003138 -0.076258  0.090029 -0.203510   \n1387 -0.025485  0.102357  0.066248  0.009761 -0.063254  0.117716 -0.217594   \n1388 -0.045263  0.039170  0.066365  0.004597 -0.094528  0.101063 -0.135450   \n1389  0.009898  0.055380  0.035460  0.043163 -0.065155  0.108556 -0.198690   \n1390 -0.081198  0.116138  0.045936 -0.022857 -0.053637  0.091898 -0.216672   \n1391  0.045502 -0.003683  0.099556  0.072249 -0.082491  0.093371 -0.184449   \n1392 -0.054983  0.064057  0.046408 -0.000474 -0.116000  0.085190 -0.196891   \n\n           7         8         9    ...       118       119       120  \\\n934  -0.123919  0.036245 -0.059159  ... -0.104611 -0.047626  0.121834   \n935  -0.023634 -0.065791 -0.150818  ... -0.102910  0.000593  0.130436   \n936  -0.080175 -0.024824 -0.121706  ... -0.136008 -0.072596  0.114800   \n937  -0.060607 -0.039211 -0.080643  ... -0.096841  0.008682  0.186951   \n938  -0.105792 -0.046862 -0.099199  ... -0.122794 -0.075878  0.124534   \n939  -0.080881  0.057372 -0.052691  ... -0.089102 -0.012204  0.114531   \n940  -0.099755 -0.063830 -0.105595  ... -0.107758  0.040046  0.053210   \n941  -0.120265 -0.070524 -0.096580  ... -0.106449 -0.023825  0.106828   \n1716  0.000992 -0.067416  0.001675  ...  0.015843 -0.043248  0.094100   \n1717 -0.003067 -0.034211  0.012615  ...  0.030201 -0.031655  0.139529   \n1718 -0.030298 -0.058000  0.031915  ...  0.034888 -0.010725  0.140663   \n1719 -0.022984 -0.074102 -0.013079  ...  0.044588 -0.036455  0.116399   \n1720 -0.007451 -0.042938  0.035795  ...  0.022369  0.001701  0.144478   \n1721 -0.005793 -0.029539  0.039453  ... -0.008435  0.014682  0.111573   \n1722 -0.037653 -0.008614  0.004209  ...  0.012572 -0.041818  0.130245   \n1723 -0.041088 -0.014929 -0.051585  ...  0.074453 -0.037160  0.082777   \n479  -0.136831  0.072510 -0.090896  ... -0.053386 -0.089758  0.080337   \n480  -0.154688 -0.013376 -0.139593  ... -0.090970 -0.054356  0.131555   \n481  -0.159251  0.036604 -0.063258  ... -0.086793 -0.043570  0.153617   \n482  -0.158155  0.062739 -0.074130  ... -0.047619 -0.068439  0.183389   \n483  -0.219204  0.060765 -0.063506  ...  0.000754 -0.050339  0.158834   \n484  -0.188793 -0.024778 -0.055408  ... -0.160470 -0.074535  0.277662   \n485  -0.048719 -0.002461  0.017123  ... -0.070967 -0.146546  0.097680   \n486  -0.110879  0.036627 -0.050492  ... -0.064658 -0.064621  0.158106   \n2249 -0.088523  0.015226  0.052932  ... -0.068847 -0.018829  0.032177   \n2250 -0.112699 -0.036781  0.011135  ... -0.083258 -0.014685  0.067983   \n2251 -0.112699 -0.036781  0.011135  ... -0.083258 -0.014685  0.067983   \n2252 -0.085727 -0.021369  0.040039  ... -0.049931 -0.009536  0.097721   \n2253 -0.045459  0.025983  0.060871  ... -0.082350 -0.011824 -0.000347   \n2254 -0.045459  0.025983  0.060871  ... -0.082350 -0.011824 -0.000347   \n2255 -0.101708 -0.009414  0.006243  ... -0.115943 -0.023348  0.076053   \n2256 -0.055885 -0.101138 -0.020614  ... -0.018505 -0.067245  0.039689   \n0    -0.014943 -0.047392 -0.143377  ... -0.052132 -0.053383  0.010852   \n1    -0.072983 -0.040834 -0.107062  ... -0.066836 -0.014427  0.082855   \n2    -0.010881 -0.078560 -0.133603  ... -0.020766 -0.084379  0.017491   \n3    -0.090109 -0.045213 -0.084140  ... -0.055584 -0.046327  0.008733   \n4    -0.019684 -0.001649 -0.143758  ... -0.026117 -0.039791  0.015301   \n5    -0.005627 -0.076807 -0.130495  ... -0.095965 -0.018726 -0.007270   \n6    -0.002576 -0.007020 -0.083703  ... -0.036458 -0.010540 -0.009035   \n7    -0.076338 -0.016801 -0.087189  ... -0.053812 -0.027023 -0.000889   \n1385  0.015671 -0.202418 -0.160448  ...  0.115527 -0.038034  0.112184   \n1386  0.045964 -0.116441 -0.126658  ...  0.133905 -0.016568  0.084965   \n1387  0.069774 -0.157148 -0.085220  ...  0.123369 -0.034499  0.093638   \n1388  0.099563 -0.121963 -0.052456  ...  0.163519 -0.087845  0.022806   \n1389  0.015911 -0.184493 -0.120757  ...  0.053628 -0.042896  0.069205   \n1390  0.074075 -0.130192 -0.087616  ...  0.115452 -0.045789  0.061476   \n1391  0.057472 -0.167865 -0.074750  ...  0.065257 -0.076379  0.043283   \n1392  0.064992 -0.209126 -0.174044  ...  0.112741 -0.070892  0.065007   \n\n           121       122       123       124       125       126       127  \n934   0.179762 -0.017347  0.008359  0.062670 -0.078373 -0.042598  0.019582  \n935   0.225266 -0.084585 -0.095831  0.043596 -0.005548 -0.031472 -0.026424  \n936   0.159348 -0.034028 -0.076331  0.090079 -0.080794 -0.071002  0.033135  \n937   0.171635 -0.029339 -0.097474  0.033675 -0.037784 -0.020224  0.004597  \n938   0.199194 -0.013245 -0.077795  0.072407 -0.045857 -0.102757 -0.022754  \n939   0.104207 -0.113281 -0.132489  0.016064 -0.025311 -0.066756  0.036115  \n940   0.161246 -0.043947 -0.051046  0.009384 -0.056961 -0.080840  0.009038  \n941   0.189305  0.025780 -0.115816  0.060286 -0.046626 -0.079291 -0.017822  \n1716  0.108066  0.061597 -0.024869 -0.042547 -0.143540 -0.010120  0.010617  \n1717  0.123770  0.008869 -0.041684 -0.069185 -0.131065  0.038842  0.018576  \n1718  0.120142  0.039925 -0.056840 -0.049773 -0.094168  0.028814  0.044170  \n1719  0.102583  0.043362 -0.042621 -0.027003 -0.119942  0.005996  0.041500  \n1720  0.101451  0.044506 -0.028225 -0.016722 -0.130686  0.002998  0.059360  \n1721  0.122225  0.026342 -0.029693 -0.027278 -0.114920  0.063081  0.009032  \n1722  0.133716  0.022856 -0.035439 -0.051431 -0.114706  0.014908 -0.017442  \n1723  0.138181 -0.023889 -0.023231 -0.019323 -0.127636 -0.025325  0.029251  \n479  -0.041748  0.090335  0.079490  0.087041 -0.104528  0.147845 -0.005830  \n480  -0.023530  0.085087  0.053814  0.031182 -0.122679  0.142670  0.028062  \n481   0.095506  0.061518  0.138263 -0.004124 -0.121992  0.140104 -0.057831  \n482   0.023841  0.033709  0.094858 -0.037080 -0.066627  0.127150 -0.093377  \n483   0.069140  0.027472  0.100979  0.020880 -0.101866  0.082393 -0.129908  \n484   0.028357  0.088527  0.136504  0.012978 -0.031560  0.181826 -0.056607  \n485  -0.129224 -0.024843  0.102420  0.004529 -0.138815 -0.058648 -0.118394  \n486  -0.064947  0.147087  0.066469  0.042794 -0.102035  0.162006  0.050235  \n2249  0.058102 -0.041591  0.111199  0.123279 -0.071850 -0.040127 -0.025230  \n2250  0.066260 -0.032890  0.055437  0.144801 -0.052387 -0.030313  0.000824  \n2251  0.066260 -0.032890  0.055437  0.144801 -0.052387 -0.030313  0.000824  \n2252  0.096457 -0.052424  0.043053  0.156550 -0.043684 -0.034680 -0.040420  \n2253  0.046023 -0.018723  0.107897  0.180427 -0.060872 -0.023672 -0.003113  \n2254  0.046023 -0.018723  0.107897  0.180427 -0.060872 -0.023672 -0.003113  \n2255  0.052539 -0.068185  0.081201  0.112583 -0.035979 -0.053197 -0.006565  \n2256  0.116662 -0.010136  0.051305  0.160871 -0.170249 -0.126635  0.015619  \n0    -0.036028 -0.032719  0.059644 -0.030630 -0.017200 -0.060757  0.100916  \n1    -0.055209 -0.033568  0.048271 -0.081447 -0.034860 -0.071171  0.110100  \n2     0.006223 -0.050118  0.048933  0.017041 -0.042428 -0.111494  0.111673  \n3    -0.155385  0.014930  0.113270 -0.052694 -0.035386 -0.121243  0.187436  \n4    -0.008482 -0.060086  0.023200 -0.023407 -0.047063 -0.079421  0.083192  \n5    -0.010116 -0.021352  0.061234 -0.042534 -0.057679 -0.073894  0.096260  \n6    -0.067008  0.086221  0.049132  0.007107 -0.086372 -0.100788  0.156235  \n7    -0.138852  0.020464  0.174709  0.003074 -0.053757 -0.083414  0.202151  \n1385 -0.075733 -0.048191  0.037778  0.083655 -0.065335 -0.048985  0.029475  \n1386 -0.104454 -0.063785  0.051138  0.100228 -0.058918 -0.044693  0.090065  \n1387 -0.095279 -0.048440  0.011371  0.077978  0.016656 -0.016332  0.063293  \n1388 -0.119003 -0.030124  0.100478  0.119466 -0.011479 -0.031744  0.217655  \n1389 -0.064782 -0.024459  0.007147  0.124954 -0.038636 -0.011503  0.069931  \n1390 -0.154865 -0.042209  0.089452  0.076045 -0.008680 -0.048226  0.165829  \n1391 -0.134399 -0.027271  0.031471  0.087447 -0.019418 -0.034423  0.052263  \n1392 -0.057210 -0.084914  0.043889  0.122770 -0.045670 -0.028864  0.060955  \n\n[48 rows x 128 columns]\ny_ten [0 0 0 ... 5 5 5]\n"
    }
   ],
   "source": [
    "# get data and labels of all indexes\n",
    "x_ten = data.loc[ all_indexes, : ]\n",
    "y_ten = target.loc[ all_indexes , : ]\n",
    "\n",
    "print(\"x_ten\", x_ten)\n",
    "print(\"y_ten\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_ten, y_ten, test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train dat lenght  28\n# Tuning hyper-parameters for precision\n\nBest parameters set found on development set:\n\n{'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n\nGrid scores on development set:\n\n0.167 (+/-0.533) for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n0.167 (+/-0.533) for {'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'}\n0.167 (+/-0.533) for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n0.167 (+/-0.533) for {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}\n0.642 (+/-0.142) for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n0.167 (+/-0.533) for {'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'}\n1.000 (+/-0.000) for {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n0.642 (+/-0.142) for {'C': 1000, 'gamma': 0.0001, 'kernel': 'rbf'}\n1.000 (+/-0.000) for {'C': 1, 'kernel': 'linear'}\n1.000 (+/-0.000) for {'C': 10, 'kernel': 'linear'}\n1.000 (+/-0.000) for {'C': 100, 'kernel': 'linear'}\n1.000 (+/-0.000) for {'C': 1000, 'kernel': 'linear'}\n1.000 (+/-0.000) for {'degree': 1, 'kernel': 'poly'}\n1.000 (+/-0.000) for {'degree': 3, 'kernel': 'poly'}\n1.000 (+/-0.000) for {'degree': 5, 'kernel': 'poly'}\n0.950 (+/-0.200) for {'degree': 7, 'kernel': 'poly'}\n\nDetailed classification report:\n\nThe model is trained on the full development set.\nThe scores are computed on the full evaluation set.\n\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00         3\n           1       1.00      1.00      1.00         2\n           2       1.00      1.00      1.00         3\n           3       1.00      1.00      1.00         3\n           4       1.00      1.00      1.00         5\n           5       1.00      1.00      1.00         4\n\n    accuracy                           1.00        20\n   macro avg       1.00      1.00      1.00        20\nweighted avg       1.00      1.00      1.00        20\n\n\n# Tuning hyper-parameters for recall\n\nBest parameters set found on development set:\n\n{'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n\nGrid scores on development set:\n\n0.307 (+/-0.494) for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n0.307 (+/-0.494) for {'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'}\n0.307 (+/-0.494) for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n0.307 (+/-0.494) for {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}\n0.747 (+/-0.131) for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n0.307 (+/-0.494) for {'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'}\n1.000 (+/-0.000) for {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n0.747 (+/-0.131) for {'C': 1000, 'gamma': 0.0001, 'kernel': 'rbf'}\n1.000 (+/-0.000) for {'C': 1, 'kernel': 'linear'}\n1.000 (+/-0.000) for {'C': 10, 'kernel': 'linear'}\n1.000 (+/-0.000) for {'C': 100, 'kernel': 'linear'}\n1.000 (+/-0.000) for {'C': 1000, 'kernel': 'linear'}\n1.000 (+/-0.000) for {'degree': 1, 'kernel': 'poly'}\n1.000 (+/-0.000) for {'degree': 3, 'kernel': 'poly'}\n1.000 (+/-0.000) for {'degree': 5, 'kernel': 'poly'}\n0.967 (+/-0.133) for {'degree': 7, 'kernel': 'poly'}\n\nDetailed classification report:\n\nThe model is trained on the full development set.\nThe scores are computed on the full evaluation set.\n\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00         3\n           1       1.00      1.00      1.00         2\n           2       1.00      1.00      1.00         3\n           3       1.00      1.00      1.00         3\n           4       1.00      1.00      1.00         5\n           5       1.00      1.00      1.00         4\n\n    accuracy                           1.00        20\n   macro avg       1.00      1.00      1.00        20\nweighted avg       1.00      1.00      1.00        20\n\n\n"
    }
   ],
   "source": [
    "# Supervised SVC parameter grid search\n",
    "n_samples = len(X_train)\n",
    "print(\"Train dat lenght \",n_samples)\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],\n",
    "                     'C': [1, 10, 100, 1000]},\n",
    "                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]},\n",
    "                    {'kernel': ['poly'], 'degree': [1, 3, 5, 7]}]\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(SVC(), tuned_parameters, scoring='%s_macro' % score)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1594971460823",
   "display_name": "Python 3.7.7 64-bit ('faceNetEmbeddingsClassifier': pipenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}