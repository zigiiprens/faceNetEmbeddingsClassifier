{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import unique\n",
    "from numpy import where\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/data.csv\",  header=None)\n",
    "target = pd.read_csv(\"data/id.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "INFO data\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 2450 entries, 0 to 2449\nData columns (total 128 columns):\n #   Column  Dtype  \n---  ------  -----  \n 0   0       float64\n 1   1       float64\n 2   2       float64\n 3   3       float64\n 4   4       float64\n 5   5       float64\n 6   6       float64\n 7   7       float64\n 8   8       float64\n 9   9       float64\n 10  10      float64\n 11  11      float64\n 12  12      float64\n 13  13      float64\n 14  14      float64\n 15  15      float64\n 16  16      float64\n 17  17      float64\n 18  18      float64\n 19  19      float64\n 20  20      float64\n 21  21      float64\n 22  22      float64\n 23  23      float64\n 24  24      float64\n 25  25      float64\n 26  26      float64\n 27  27      float64\n 28  28      float64\n 29  29      float64\n 30  30      float64\n 31  31      float64\n 32  32      float64\n 33  33      float64\n 34  34      float64\n 35  35      float64\n 36  36      float64\n 37  37      float64\n 38  38      float64\n 39  39      float64\n 40  40      float64\n 41  41      float64\n 42  42      float64\n 43  43      float64\n 44  44      float64\n 45  45      float64\n 46  46      float64\n 47  47      float64\n 48  48      float64\n 49  49      float64\n 50  50      float64\n 51  51      float64\n 52  52      float64\n 53  53      float64\n 54  54      float64\n 55  55      float64\n 56  56      float64\n 57  57      float64\n 58  58      float64\n 59  59      float64\n 60  60      float64\n 61  61      float64\n 62  62      float64\n 63  63      float64\n 64  64      float64\n 65  65      float64\n 66  66      float64\n 67  67      float64\n 68  68      float64\n 69  69      float64\n 70  70      float64\n 71  71      float64\n 72  72      float64\n 73  73      float64\n 74  74      float64\n 75  75      float64\n 76  76      float64\n 77  77      float64\n 78  78      float64\n 79  79      float64\n 80  80      float64\n 81  81      float64\n 82  82      float64\n 83  83      float64\n 84  84      float64\n 85  85      float64\n 86  86      float64\n 87  87      float64\n 88  88      float64\n 89  89      float64\n 90  90      float64\n 91  91      float64\n 92  92      float64\n 93  93      float64\n 94  94      float64\n 95  95      float64\n 96  96      float64\n 97  97      float64\n 98  98      float64\n 99  99      float64\n 100 100     float64\n 101 101     float64\n 102 102     float64\n 103 103     float64\n 104 104     float64\n 105 105     float64\n 106 106     float64\n 107 107     float64\n 108 108     float64\n 109 109     float64\n 110 110     float64\n 111 111     float64\n 112 112     float64\n 113 113     float64\n 114 114     float64\n 115 115     float64\n 116 116     float64\n 117 117     float64\n 118 118     float64\n 119 119     float64\n 120 120     float64\n 121 121     float64\n 122 122     float64\n 123 123     float64\n 124 124     float64\n 125 125     float64\n 126 126     float64\n 127 127     float64\ndtypes: float64(128)\nmemory usage: 2.4 MB\nNone\nINFO target\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 2450 entries, 0 to 2449\nData columns (total 1 columns):\n #   Column  Non-Null Count  Dtype\n---  ------  --------------  -----\n 0   0       2450 non-null   int64\ndtypes: int64(1)\nmemory usage: 19.3 KB\nNone\n"
    }
   ],
   "source": [
    "#print(data.head)\n",
    "print(\"INFO data\")\n",
    "print(data.info(verbose=True))\n",
    "print(\"INFO target\")\n",
    "print(target.info(verbose=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "INFO data train ndim  2\nINFO data train shape  (2450, 128)\nINFO data target ndim  1\nINFO data target shape  (2450,)\n"
    }
   ],
   "source": [
    "# Data train into numpy\n",
    "X = np.array(data.values)\n",
    "print(\"INFO data train ndim \", X.ndim)\n",
    "print(\"INFO data train shape \", X.shape)\n",
    "# Data target into numpy\n",
    "y = np.array(target.values)\n",
    "y = le.fit_transform(y)\n",
    "print(\"INFO data target ndim \", y.ndim)\n",
    "print(\"INFO data target shape \", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fake test data\n",
    "# X_test = X_train\n",
    "# y_test = y_train\n",
    "\n",
    "# Real test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "X_train  [[-0.13532428 -0.10226692 -0.230257   ... -0.05163977  0.0673781\n   0.06518991]\n [ 0.00898883 -0.00948986  0.0780158  ... -0.03504066 -0.03041432\n  -0.0091842 ]\n [-0.10288385 -0.06053137 -0.17722704 ... -0.15629248  0.03092726\n  -0.03530539]\n ...\n [-0.16807067  0.11799911 -0.09508488 ... -0.0254995  -0.05249912\n   0.12057748]\n [-0.12545732  0.13223188 -0.01288609 ... -0.03367175 -0.06297211\n   0.02498487]\n [-0.09136044  0.01424895  0.03772344 ... -0.11064813  0.14908282\n   0.03215077]]\ny_train  [4 5 4 ... 2 2 1]\n"
    }
   ],
   "source": [
    "print(\"X_train \", X_train)\n",
    "print(\"y_train \", y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "# Tuning hyper-parameters for precision\n\nBest parameters set found on development set:\n\n{'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n\nGrid scores on development set:\n\n0.035 (+/-0.000) for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n0.035 (+/-0.000) for {'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'}\n0.991 (+/-0.007) for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n0.035 (+/-0.000) for {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}\n0.992 (+/-0.007) for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n0.991 (+/-0.007) for {'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'}\n0.996 (+/-0.005) for {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n0.992 (+/-0.007) for {'C': 1000, 'gamma': 0.0001, 'kernel': 'rbf'}\n0.995 (+/-0.006) for {'C': 1, 'kernel': 'linear'}\n0.995 (+/-0.009) for {'C': 10, 'kernel': 'linear'}\n0.995 (+/-0.009) for {'C': 100, 'kernel': 'linear'}\n0.995 (+/-0.009) for {'C': 1000, 'kernel': 'linear'}\n0.995 (+/-0.006) for {'degree': 1, 'kernel': 'poly'}\n0.995 (+/-0.007) for {'degree': 3, 'kernel': 'poly'}\n0.994 (+/-0.010) for {'degree': 5, 'kernel': 'poly'}\n0.992 (+/-0.013) for {'degree': 7, 'kernel': 'poly'}\n\nDetailed classification report:\n\nThe model is trained on the full development set.\nThe scores are computed on the full evaluation set.\n\n              precision    recall  f1-score   support\n\n           0       1.00      0.99      1.00       170\n           1       1.00      1.00      1.00       155\n           2       1.00      0.99      1.00       136\n           3       0.99      1.00      1.00       106\n           4       0.99      1.00      1.00       188\n           5       1.00      1.00      1.00        54\n\n    accuracy                           1.00       809\n   macro avg       1.00      1.00      1.00       809\nweighted avg       1.00      1.00      1.00       809\n\n\n# Tuning hyper-parameters for recall\n\nBest parameters set found on development set:\n\n{'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n\nGrid scores on development set:\n\n0.167 (+/-0.000) for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n0.167 (+/-0.000) for {'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'}\n0.988 (+/-0.010) for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n0.167 (+/-0.000) for {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}\n0.988 (+/-0.010) for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n0.988 (+/-0.010) for {'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'}\n0.995 (+/-0.007) for {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n0.988 (+/-0.010) for {'C': 1000, 'gamma': 0.0001, 'kernel': 'rbf'}\n0.992 (+/-0.008) for {'C': 1, 'kernel': 'linear'}\n0.994 (+/-0.010) for {'C': 10, 'kernel': 'linear'}\n0.994 (+/-0.010) for {'C': 100, 'kernel': 'linear'}\n0.994 (+/-0.010) for {'C': 1000, 'kernel': 'linear'}\n0.992 (+/-0.008) for {'degree': 1, 'kernel': 'poly'}\n0.994 (+/-0.009) for {'degree': 3, 'kernel': 'poly'}\n0.992 (+/-0.012) for {'degree': 5, 'kernel': 'poly'}\n0.990 (+/-0.016) for {'degree': 7, 'kernel': 'poly'}\n\nDetailed classification report:\n\nThe model is trained on the full development set.\nThe scores are computed on the full evaluation set.\n\n              precision    recall  f1-score   support\n\n           0       1.00      0.99      1.00       170\n           1       1.00      1.00      1.00       155\n           2       1.00      0.99      1.00       136\n           3       0.99      1.00      1.00       106\n           4       0.99      1.00      1.00       188\n           5       1.00      1.00      1.00        54\n\n    accuracy                           1.00       809\n   macro avg       1.00      1.00      1.00       809\nweighted avg       1.00      1.00      1.00       809\n\n\n"
    }
   ],
   "source": [
    "# Supervised SVC parameter grid search\n",
    "n_samples = len(X_train)\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],\n",
    "                     'C': [1, 10, 100, 1000]},\n",
    "                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]},\n",
    "                    {'kernel': ['poly'], 'degree': [1, 3, 5, 7]}]\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(SVC(), tuned_parameters, scoring='%s_macro' % score)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "# Tuning hyper-parameters for precision\n\nBest parameters set found on development set:\n\n{'C': 10, 'kernel': 'linear'}\n\nGrid scores on development set:\n\n0.035 (+/-0.000) for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n0.035 (+/-0.000) for {'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'}\n0.991 (+/-0.008) for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n0.035 (+/-0.000) for {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}\n0.992 (+/-0.007) for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n0.991 (+/-0.008) for {'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'}\n0.992 (+/-0.007) for {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n0.992 (+/-0.007) for {'C': 1000, 'gamma': 0.0001, 'kernel': 'rbf'}\n0.992 (+/-0.007) for {'C': 1, 'kernel': 'linear'}\n0.992 (+/-0.006) for {'C': 10, 'kernel': 'linear'}\n0.991 (+/-0.010) for {'C': 100, 'kernel': 'linear'}\n0.990 (+/-0.009) for {'C': 1000, 'kernel': 'linear'}\n\nDetailed classification report:\n\nThe model is trained on the full development set.\nThe scores are computed on the full evaluation set.\n\n              precision    recall  f1-score   support\n\n           0       1.00      0.99      1.00       170\n           1       0.96      1.00      0.98       155\n           2       1.00      1.00      1.00       136\n           3       1.00      0.94      0.97       106\n           4       1.00      1.00      1.00       188\n           5       1.00      1.00      1.00        54\n\n    accuracy                           0.99       809\n   macro avg       0.99      0.99      0.99       809\nweighted avg       0.99      0.99      0.99       809\n\n\n# Tuning hyper-parameters for recall\n\nBest parameters set found on development set:\n\n{'C': 100, 'kernel': 'linear'}\n\nGrid scores on development set:\n\n0.167 (+/-0.000) for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n0.167 (+/-0.000) for {'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'}\n0.987 (+/-0.011) for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n0.167 (+/-0.000) for {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}\n0.988 (+/-0.010) for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n0.987 (+/-0.011) for {'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'}\n0.988 (+/-0.010) for {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n0.988 (+/-0.010) for {'C': 1000, 'gamma': 0.0001, 'kernel': 'rbf'}\n0.988 (+/-0.010) for {'C': 1, 'kernel': 'linear'}\n0.989 (+/-0.008) for {'C': 10, 'kernel': 'linear'}\n0.989 (+/-0.009) for {'C': 100, 'kernel': 'linear'}\n0.988 (+/-0.007) for {'C': 1000, 'kernel': 'linear'}\n\nDetailed classification report:\n\nThe model is trained on the full development set.\nThe scores are computed on the full evaluation set.\n\n              precision    recall  f1-score   support\n\n           0       1.00      0.99      1.00       170\n           1       0.97      0.99      0.98       155\n           2       1.00      1.00      1.00       136\n           3       0.99      0.97      0.98       106\n           4       1.00      1.00      1.00       188\n           5       1.00      1.00      1.00        54\n\n    accuracy                           0.99       809\n   macro avg       0.99      0.99      0.99       809\nweighted avg       0.99      0.99      0.99       809\n\n\n"
    }
   ],
   "source": [
    "# Supervised PCA + SVC parameter grid search\n",
    "\n",
    "pca = PCA(n_components=6)# adjust yourself\n",
    "pca.fit(X_train)\n",
    "X_t_train = pca.transform(X_train)\n",
    "X_t_test = pca.transform(X_test)\n",
    "#clf = SVC()\n",
    "#clf.fit(X_t_train, y_train)\n",
    "#print 'score', clf.score(X_t_test, y_test)\n",
    "#print 'pred label', clf.predict(X_t_test)\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],\n",
    "                     'C': [1, 10, 100, 1000]},\n",
    "                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(SVC(probability=True), tuned_parameters, scoring='%s_macro' % score)\n",
    "    clf.fit(X_t_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, clf.predict(X_t_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[7.33437424e-04 5.07193533e-04 9.95301521e-01 8.47333012e-04\n  1.30504202e-03 1.30547324e-03]\n [1.32956349e-03 9.23167878e-04 9.95122689e-01 4.32750975e-04\n  7.74480406e-04 1.41734854e-03]\n [1.63906573e-05 1.17100003e-04 1.82861290e-03 3.80974046e-04\n  9.58971882e-04 9.96697951e-01]\n ...\n [7.42363192e-03 1.74432066e-03 9.83707741e-01 1.53244265e-03\n  1.20706651e-03 4.38479774e-03]\n [2.83747761e-03 1.24321954e-03 1.53856789e-03 9.92093645e-01\n  1.24726734e-03 1.03982294e-03]\n [5.33618611e-04 8.78065265e-01 1.00941109e-02 9.59279607e-02\n  2.65632140e-03 1.27227230e-02]]\n"
    }
   ],
   "source": [
    "# Predict proba\n",
    "y_true, y_pred = y_test, clf.predict_proba(X_t_test)\n",
    "print( y_pred )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DBSCAN\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics\n",
    "\n",
    "# #############################################################################\n",
    "# Compute DBSCAN\n",
    "db = DBSCAN(eps=0.7, min_samples=100).fit(X_train)\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "labels = db.labels_\n",
    "\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "\n",
    "print('Estimated number of clusters: %d' % n_clusters_)\n",
    "print('Estimated number of noise points: %d' % n_noise_)\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(X_train, labels))\n",
    "print('Estimated clusters ', labels)\n",
    "\n",
    "# Plot result\n",
    "\n",
    "# Black removed and is used for noise instead.\n",
    "unique_labels = set(labels)\n",
    "colors = [plt.cm.Spectral(each)\n",
    "          for each in np.linspace(0, 1, len(unique_labels))]\n",
    "for k, col in zip(unique_labels, colors):\n",
    "    if k == -1:\n",
    "        # Black used for noise.\n",
    "        col = [0, 0, 0, 1]\n",
    "\n",
    "    class_member_mask = (labels == k)\n",
    "\n",
    "    xy = X_train[class_member_mask & core_samples_mask]\n",
    "    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
    "             markeredgecolor='k', markersize=14)\n",
    "\n",
    "    xy = X_train[class_member_mask & ~core_samples_mask]\n",
    "    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
    "             markeredgecolor='k', markersize=6)\n",
    "\n",
    "plt.title('Estimated number of clusters: %d' % n_clusters_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BIRCH\n",
    "from sklearn.cluster import Birch\n",
    "\n",
    "# define the model\n",
    "model = Birch(threshold=0.1, n_clusters=7)\n",
    "# fit the model\n",
    "model.fit(X_train)\n",
    "# assign a cluster to each example\n",
    "yhat = model.predict(X_train)\n",
    "# retrieve unique clusters\n",
    "clusters = unique(yhat)\n",
    "# create scatter plot for samples from each cluster\n",
    "for cluster in clusters:\n",
    "    # get row indexes for samples with this cluster\n",
    "    row_ix = where(yhat == cluster)\n",
    "    # create scatter of these samples\n",
    "    plt.scatter(X_train[row_ix, 0], X_train[row_ix, 1])\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kmeans\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.cluster import KMeans\n",
    "from matplotlib import pyplot\n",
    "# define the model\n",
    "model = KMeans(n_clusters=7)\n",
    "# fit the model\n",
    "model.fit(X_train)\n",
    "# assign a cluster to each example\n",
    "yhat = model.predict(X_train)\n",
    "# retrieve unique clusters\n",
    "clusters = unique(yhat)\n",
    "# create scatter plot for samples from each cluster\n",
    "for cluster in clusters:\n",
    "    # get row indexes for samples with this cluster\n",
    "    row_ix = where(yhat == cluster)\n",
    "    # create scatter of these samples\n",
    "    pyplot.scatter(X_train[row_ix, 0], X_train[row_ix, 1])\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optics clustering\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.cluster import OPTICS\n",
    "from matplotlib import pyplot\n",
    "# define the model\n",
    "model = OPTICS(eps=0.8, min_samples=100)\n",
    "# fit model and predict clusters\n",
    "yhat = model.fit_predict(X_train)\n",
    "# retrieve unique clusters\n",
    "clusters = unique(yhat)\n",
    "# create scatter plot for samples from each cluster\n",
    "for cluster in clusters:\n",
    "    # get row indexes for samples with this cluster\n",
    "    row_ix = where(yhat == cluster)\n",
    "    # create scatter of these samples\n",
    "    pyplot.scatter(X_train[row_ix, 0], X_train[row_ix, 1])\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gaussian mixture clustering\n",
    "from numpy import unique\n",
    "from numpy import where\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from matplotlib import pyplot\n",
    "# define the model\n",
    "model = GaussianMixture(n_components=6)\n",
    "# fit the model\n",
    "model.fit(X_train)\n",
    "# assign a cluster to each example\n",
    "yhat = model.predict(X_train)\n",
    "# retrieve unique clusters\n",
    "clusters = unique(yhat)\n",
    "# create scatter plot for samples from each cluster\n",
    "for cluster in clusters:\n",
    "    # get row indexes for samples with this cluster\n",
    "    row_ix = where(yhat == cluster)\n",
    "    # create scatter of these samples\n",
    "    pyplot.scatter(X_train[row_ix, 0], X_train[row_ix, 1])\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Feature Selection is a technique which is used when we you know the target variable(Supervised Learning)\n",
    "When we talk with respect to Unsupervised Learning, there is no exact technique which could do that. But there is something which can help us in those lines i.e., Dimensionality Reduction, this technique is used to reduce the number of features and give us the features which explains the most about the dataset. The features would be derived from the existing features and might or might not be the same features.\n",
    "There are different techniques which are available for doing so:\n",
    "\n",
    "# PCA\n",
    "# Linear discriminant analysis\n",
    "Non-negative Matrix Factorization\n",
    "Generalized discriminant analysis and many more.\n",
    "The outcome of Feature Selection would be the same features which explain the most with respect to the target variable but the outcome of the Dimensionality Reduction might or might not be the same features as these are derived from the given input.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PCA\n",
    "from sklearn import decomposition\n",
    "pca = decomposition.PCA(n_components=6)\n",
    "X_reduced = pca.fit_transform(X_train)\n",
    "\n",
    "print('Projecting %d-dimensional data to 2D' % X_train.shape[1])\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y_train, label=y_train,\n",
    "            edgecolor='none', alpha=0.7, s=40,\n",
    "            cmap=plt.cm.get_cmap('nipy_spectral', 10))\n",
    "plt.colorbar()\n",
    "plt.title(' PCA projection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# gaussian mixture clustering after PCA\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from matplotlib import pyplot\n",
    "# define the model\n",
    "model = GaussianMixture(n_components=6)\n",
    "# fit the model\n",
    "model.fit(X_reduced)\n",
    "# assign a cluster to each example\n",
    "yhat = model.predict(X_reduced)\n",
    "# retrieve unique clusters\n",
    "clusters = unique(yhat)\n",
    "# create scatter plot for samples from each cluster\n",
    "for cluster in clusters:\n",
    "    # get row indexes for samples with this cluster\n",
    "    row_ix = where(yhat == cluster)\n",
    "    # create scatter of these samples\n",
    "    pyplot.scatter(X_reduced[row_ix, 0], X_reduced[row_ix, 1])\n",
    "# show the plot\n",
    "pyplot.show()\n",
    "print(yhat[100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('faceNetEmbeddingsClassifier': pipenv)",
   "language": "python",
   "name": "python_defaultSpec_1594971460857"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}